[{"title":"非参数估计","url":"/2022/01/10/%E9%9D%9E%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1/","content":"在之前的学习中，我们总是假设概率密度函数的参数形式已知，并在此条件下处理有监督学习过程。而在现实世界中，我们给出的概率密度函数很少符合实际情况，本节我们讨论**非参数化方法(non-parametric method)**，它可以处理任意的概率分布而不必假设密度的参数形式已知。大体上还是遵循着贝叶斯决策论，主要有两个非参数估计的方向：\n\n从训练样本中估计类条件概率密度：\n直接估计后验概率：\n\n[TOC]\n概率密度的估计估计类条件概率密度的最基本的一个条件就是：一个向量x落在区域中的概率为是在区域上关于的导数的积分（相当于区域R内每个点的概率密度函数的积分，概率密度函数的积分就是概率函数），因此概率是概率密度函数的平滑（或取平均）的版本，因此我们可以通过概率来估计概率密度函数。\n先假设是连续的，并且区域足够小，以至于在这个区间中条件概率密度几乎没有变化，若其中表示区域所包含的体积（二维面积，三维代表体积）有：假设n个样本都是根据概率密度函数独立同分布(i,i,d)的抽取而得到的，其中有k个样本落在区域中的概率服从二项式定理：当样本足够大时，综合(2)式与(3)式，我们能够得到的估计为：让代表区域中样本的确切数量，有：\n\n推导过程如下图所示：\n\n\n类比利用频率直方图估计概率密度函数，理论上当我们的样本足够多，同时频率分布直方图组距设置的特别小，就是在逼近样本点的真实概率密度函数【2】：\n\n\n\n\n为了估计点处的概率密度函数，构造了一系列的包含的区域，其中第一个区域使用一个样本，第二个区域使用2个样本….，记为区域的体积，为落在中的样本的个数，而表示对的第n次估计：要求估计的概率密度函数收敛到真实值：必须满足以下三个条件，以及他们分别代表的意义【3】：\n\n\n\n随着样本数量的增加，体积尽可能小，类比频率分布直方图的组距尽可能小\n在小区域内有足够多的样本，保证频率之比能够收敛到概率\n在小区域内的样本数在总样本中所占的比例是很小的一部分\n\n\n有两种经常采用的估计途径：\n\nParzen窗：根据某一个确定的体积函数，比如，来逐渐收缩一个给定的初始区间。（要求，能保证能收敛到）\nK-nearest-neighbor：确定为n的某个函数，比如，这样体积必须逐渐生长，直到最后能包进的个相邻点。\n\n这两种方法最终都能收敛到真实概率，但在有限样本下效果不好\n\n\nParzen窗方法原理假设区间是一个d维的超立方体，如果表示超立方体一条边的长度，则体积为：，通过定义最简单的方型窗函数，得到训练集中的点是否落在落在窗中：\n$$\\varphi(\\frac{\\textbf{x}-\\textbf{x}’}{h_n})=\\left{其他\\right.$$该窗函数定义了一个d维空间中，中心点在点的超立方体。因此代表超立方体中的样本个数是：\n\n\n代入公式（11）得到Parzen窗概率密度函数：\n\n\n窗口函数本质上是出现在该区域内部的采样点的加权频数，可以看做是一种平滑。更一般的，窗函数并不一定是超立方体定义的函数，它可以是任意形式只要保证：\n\n\n\n另一个常见的高斯窗口函数如下：\n\n\n高斯窗口函数，随着采样点与中心点距离的增大而减小。\n当采用高斯窗口函数时，是出现在该区域内部的采样点的加权频数，每个采样点的权重取决于它们与中心的距离。\n\n回过头来看Parzen窗估计的概率密度函数（x代表测试样本，xi代表训练集数据）：表示我们对的估计是对一系列关于和的函数做平均，在本质上，是每一个样本依据它离的远近不同而对结果做出不同贡献。而且在进行估计前要确定一个具体形式的函数\n窗宽的影响我们定义如下，可以重写：\n\n\n因为，窗宽会显著影响的振幅与宽度\n\n\n\n\n\n如果非常大，那么的影响就很低，即使距离很远，和的差别也不大，这种情况下，是n个宽的、满变的函数的叠加，因此非常平滑\n\n如果很小，的峰值就非常大，这种情况下，是n个以样本为中心的尖脉冲的叠加，也就是一个充满噪声（不确定性）的估计\n\n\n\n\n\n的收敛性： 是样本点（随机变量）的函数， 所以我们希望随机变量的均值和方差满足：\n\n\n经过证明需要满足以下条件，证明过程可参考【4】：\n\n\n这就告诉我们对于的选择，当n趋向于正无穷时趋近于0，但必须以低于1/n的速率，因此常设定或\n\n总结：对于窗宽（或）的选取在很大程度上影响。如果太大，那么估计结果的分辨率就太低，如果太小，那么估计结果的统计稳定性就不够。当样本有限时，尽可能的取折中的估计，然而当样本个数无限，那么就可以在n增加时，让缓缓趋近于0，这样就收敛到某个概率密度函数\n下面举一个例子说明窗宽对估计结果的影响：\n\n\n\n\n下图是一个二维Prazen窗的两类分类器，左图是小窗宽，右图为大窗宽\n\n\nK-Nearest Neighbor方法在Parzen窗中，我们固定了体积求解，但最佳的窗函数的选择总是一个问题。另一种思路是固定，让体积进行变化，而不是硬性的规定窗函数为落在区域内的全体样本个数的某个函数。比如我们以样本点为中心，让体积扩张，直到包含个样本为止（是关于n的某个特定函数）。这些样本称为点的个最近邻，带入（一）中的非参数估计公式：\n后验概率的估计假设我们把一个体积放在点周围，并且能够包含进k个样本，其中个属于类别。对于联合概率密度的估计为：这样对后验概率的估计就是：这里的值可以由Parzen窗与K近邻决定。\n\nParzen窗方法中，必须是关于n的某个固定形式的函数，比如或\nKn最近邻方法中，必须保证能够包含进足够的样本个数，比如\n\n如果有足够多的样本点并且体积足够小就可以证明这样处理是比较准确的。\n最近邻规则最近邻可以看作的k=1情况下的K近邻方法，最近邻规则描述了我们只依赖某个的单一的最近的邻居来做估计，也能达到足够好的性能：令，每个样本已标记，对于测试样本点，在集合D中距离它最近的点的类别为样本点的类别。\n最近邻规则是次优的方法，通常的误差率比贝叶斯误差要高，然而在无限训练样本的情况下，这个误差率至多不会超过贝叶斯误差率的两倍。\n通过最近邻规则，我们可以把样本特征空间分为一个个小的单元格(cell)，每个单元格中的点到最近邻的距离都比到别的样本点的距离要小。这个小单元格中的任意点的类别就与最近邻的类别相同。被称为：空间Voronoi网格。\n\n\n将相同类型的网格的边界相连就可以形成决策边界，最近邻的边界通常是不平滑的，我们可以通过扩大K值来去除噪音，平滑边界。\n\n\n参考【1】模式识别\n【2】非参数估计_音程的博客-CSDN博客_非参数估计\n【3】非参数估计 - 简书 (jianshu.com)\n【4】经典的非参估计 （二）parzen 窗 - 知乎 (zhihu.com)\n","categories":["机器学习"],"tags":["机器学习","非参数估计"]},{"title":"最大似然估计与贝叶斯估计","url":"/2022/01/10/%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1%E4%B8%8E%E8%B4%9D%E5%8F%B6%E6%96%AF%E4%BC%B0%E8%AE%A1/","content":"[TOC]\n参数估计在贝叶斯决策论中，我们已经学习了如何根据先验概率与类条件概率密度来设计最优分类器。但在实际应用中，通常得不到有关问题的概率结构的全部信息。通常的解决方案是利用这些训练样本来估计问题中所涉及的先验概率和类条件密度函数，并把这些估计的结果当做实际问题的先验概率和类条件概率密度，然后在设计分类器\n在典型的监督学习问题中，有标注的样本估计先验概率不困难，最大的困难在于估计类条件概率密度：\n\n已有的训练样本数太少，很难满足所有的特征都存在的情况\n当用于表示特征的向量x的维数较大时，就会产生严重的计算复杂度问题（算法的执行时间，系统的资源开销..）\n\n但如果先验知识允许我们把条件概率密度进行参数化，例如：我们可以假设是一个多元高斯分布，其均值是，协方差矩阵为。这样我们就把问题从估计完全未知的类条件概率密度转化为了估计参数与。这样的方法被称为参数估计方法。与之对应的也有非参数估计方法。\n参数估计问题是统计学中的经典问题，主要的解决方案有两种，分别对应统计学中的两大学派：\n\n最大似然估计——频率学派（Frequentist）\n贝叶斯估计——贝叶斯学派（Bayesian）\n\n当然，在参数估计完成后，我们仍然使用后验概率作为分类准则。\n最大似然估计参数分量根据每个样本所属的类别对样本集进行分类：，任意一个分类样本集中的样本都是独立的根据类条件概率密度函数来抽取的。因此获得一个重要假设：**每个样本集中的样本都是独立同分布的随机变量(independent and identically distributed：i.i.d)**，我们还假设每一个类的类条件概率密度的形式都是已知的，未知的是具体的参数向量的值。比如：假设服从多维正态分布，这样向量就由分量所组成。\n为了强调类条件概率密度函数依赖于参数向量的情况，通常写为形如的形式。因此最大似然估计（MLE）解决的就是正确的估计各个类别的具体的参数向量：。\n基本原理问题其实就变成了一个条件概率最大的求解，即在已有的训练集的条件下，求使得 最大的参数，形式化表达为求解而根据条件概率公式有因为我们在极大似然估计中假设是确定的，所以就是一个常数。同样是根据已有的数据得到的，也是确定的，或者我们可以把其看作是对整个概率的一个归一化因子。这时候，求解公式 (1) 就变成了求解一个有监督的总的样本集分为c类，我们要估计每个类的后验概率，有c个独立的问题，每个问题解决思路相同如下：已知样本集中有n个样本。由于这些样本独立同分布，因此由（3）中的似然函数得：【2】因为样本集已知，可以看作是参数向量的函数，**参数向量的最大似然估计，就是使似然函数：达到最大值的参数向量，记为**。为了方便分析取对数，定义对数似然函数：\n如果实际的待求参数的个数为p，则参数向量可以写成如下的p维向量的形式：。记为梯度算子（构建求偏导数的数学式）这样用数学语言表示求的全过程：$$\\nabla_\\theta=\\l(\\theta|D)=ln\\sum_{k=1}^np(\\textbf{x}k|\\theta)\\计算：\\hat{\\theta}=\\underset{\\theta}{argmax};l(\\theta|D)\\等价于： \\nabla_\\theta l=\\sum{k=1}^n\\nabla_\\theta lnp(\\textbf{x}_k|\\theta)=0$$注意：求得的解可能是全局最大值点，也可能是局部极值点。\n高斯情况深入讨论当训练样本服从多元正态分布时的情况。\n协方差矩阵已知，而均值未知\n\n\n\n我们得到以下结论，对于均值的最大似然估计就是对全体样本取平均。\n协方差矩阵和均值都未知高斯分布的更典型的情况是协方差矩阵和均值都未知。这样参数向量由两个分量组成。\n\n先考虑单变量的情况\n  \n\n  \n当高斯函数是多元时，最大似然估计的结果是：  \n\n\n**均值的最大似然估计就是样本的均值，而协方差的最大似然估计是n个$(\\textbf{x}_k-\\hat{\\mu})(\\textbf{x}_k-\\hat{\\mu})^t$的算术平均。**实际上对方差的最大似然估计是有偏的估计，样本的协方差矩阵$C=\\frac{1}{n-1}(\\textbf{x}_k-\\hat{\\mu})(\\textbf{x}_k-\\hat{\\mu})^t$，而我们估计的方差是$\\hat{\\sigma}=\\frac{n-1}{n}C$\n\n&gt; 样本协方差矩阵的推导过程【2】：\n&gt;\n&gt; &lt;img src=\"最大似然估计与贝叶斯估计.assets\\dff0de718d34e83e9fd3f6c931675889_720w.jpg\" alt=\"img\" style=\"zoom:80%;\" /&gt;\n\n贝叶斯估计基本原理\nMLP通过最大化似然函数从而找出参数，思想在于找出能最大概率生成这批数据的参数。但是这种做法完全依赖于数据本身，当数据量大的时候，最大似然估计往往能很好的估计出参数 ；但是当数据量小的时候，估计出来的结果并不会很好。就例如丢硬币问题，当我们投掷了5次，得出了正正正正正的结果，极大似然估计会得出投出正面的概率为100%！这显然是不符常理的。\n贝叶斯派的人认为，后验概率中被估计的参数同样服从一种已知的分布，即参数也为一个随机变量。他们在估计参数前会先带来先验知识，例如参数在[0.5,0.6]的区域内出现的概率最大，在引入了先验知识后利用样本估计出参数分布的形式，在数据量小的情况下估计出来的结果往往会更合理。【2】\n\n我们希望利用现有的全部信息来为测试样本x计算分类的依据：后验概率（现有的全部信息一部分为我们的先验知识，比如未知概率密度函数的形式，未知参数取值范围；另一部分则来自训练集本身），假设已有训练集，改写后验概率为：表示训练集的重要性，则贝叶斯公式为：\n\n这一公式指出我们能够根据训练样本提供的信息来确定类条件概率密度和先验概率。\n\n通常认为先验概率可以事先得到，或简单计算得到，可以简写为\n\n有监督学习可以把每个样本都归到它所属的类中，如果，那么样本集中的训练样本就对没有任何影响，这样就产生两个简化：\n\n能够对每一个类分别处理：只是用中的训练样本就能确定\n能够对每个类进行分别处理，公式中说明类别的符号都可以省略\n\n  本质上我们处理的是c个独立的问题，每个问题形式：已知一组训练样本D，根据这些样本估计（未简化为）\n\n\n\n参数估计法的前提是每个类的类条件概率密度形式已知，参数向量未知。对于任意一个测试样本x，去除类符号简化为且形式已知。而贝叶斯估计与最大似然估计的区别体现在，我们不认为是一个固定但未知的值，而是服从一个概率分布。根据、、与，我们可以得到未知参数的后验概率密度函数其中【这里是训练样本集的某一个样本】接着根据积分估算测试样本的后验概率：由于对测试样本x和训练样本集D的选取是独立进行的，，公式(10)改为：\n总结整个贝叶斯估计分为三个阶段：\n\n根据样本与先验知识求得参数的后验概率\n根据参数的后验概率与类条件概率密度的形式求得每个类的具体的类条件概率密度\n最后求出每个类的后验概率\n\n\n\n\n高斯情况下的贝叶斯估计对高斯正态分布的情况，用贝叶斯估计的方法来计算未知参数的后验概率密度函数和设计分类器所需的类条件概率密度函数，其中假设（相当于第一步中的）\n单变量情况第一步：先考虑只有均值未知的情况，同时假设特征向量是一维的。，而且我们认为所有关于均值的先验知识都在先验概率密度函数中，且均值服从（已知），根据3.1节总结的贝叶斯估计三步走：\n\n\n\n\n我们发现是一个指数函数，且指数部分是的二次型，因此实际上任然是正态分布，把写成下面的形式：\n\n\n利用对应位置系数相等的原则就可以求出的值：\n\n\n上述方程显示了先验知识和样本观测结果是如何被结合在一起，并且形成的后验概率密度函数的，总的来说：\n\n代表在观测n个样本后对的真实值的最好估计，是与的线性组合\n\n表示对这种估计的不确定性（取值更离散，不集中）\n\n如果，当n趋近于，趋近于样本均值，\n当时，，这是一种退化的情况：我们对先验估计是如此的确信，以至于任何观测样本都无法改变我们的认知。\n当远大于，表示我们对先验估计如此的不确信以至于直接拿样本均值当做\n\n  根据公式（7），是n的单调递减函数，也就是说每观测一个样本，我们对的估计的不确定性就会减小，当增加时，的概率密度图就会变得更尖锐，当n趋近于无穷大时，逼近狄拉克函数。\n\n\n\n狄拉克δ函数是一个广义函数，在物理学中常用其表示质点、点电荷等理想模型的密度分布，该函数在除了零以外的点取值都等于零，而其在整个定义域上的积分等于1。\n狄拉克δ函数在概念上，它是这么一个“函数”：在除了零以外的点函数值都等于零，而其在整个定义域上的积分等于1。【4】\n\n\n\n单变量情况第二步：在得到均值的后验概率密度后，我们就可以计算类条件概率密度（实际上是，把c个问题独立，去除类有关变量后）\n\n\n\n\n求得类条件概率密度后再带入到公式(19)就能求出后验概率了。\n最大似然估计与贝叶斯估计比较最大似然估计与贝叶斯估计在训练样本趋于无穷时效果是一样的，然而在实际的问题中，训练样本总是有限的，决定我们选择哪个方法的主要因素有\n\n计算复杂度：MLE只涉及一些微分与求极值，而贝叶斯估计计算复杂的积分\n可理解性：MLE是基于设计者所提供的训练样本的一个最佳解答，而贝叶斯估计方法得到的结果则是许多可行解的加权平均值，反映出对各种可行解的不确定程度\n对初始先验知识的信任程度：MLE得到的类条件概率密度形式与先验的假设是一样的，而贝叶斯估计得到的形式可能与初始假设不相同，贝叶斯方法能比MLE利用更多有用的信息。\n\n总结贝叶斯方法有很强的理论基础，但在实际应用中最大似然估计更加简便，且性能也相差无几。\n参考【1】模式分类（第二版）\n【2】(1条消息) 极大似然估计与贝叶斯估计_Jim_Liu-CSDN博客_极大似然估计和贝叶斯估计\n【3】(20 封私信 / 29 条消息) 为什么样本方差（sample variance）的分母是 n-1？ - 知乎 (zhihu.com)\n【4】百度百科：狄拉克δ函数\n","categories":["机器学习"],"tags":["机器学习","参数估计"]},{"title":"正态密度下的分类器","url":"/2022/01/10/%E6%AD%A3%E6%80%81%E5%AF%86%E5%BA%A6%E4%B8%8B%E7%9A%84%E5%88%86%E7%B1%BB%E5%99%A8/","content":""}]