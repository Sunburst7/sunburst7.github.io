[{"title":"非参数估计","url":"/2022/01/10/%E9%9D%9E%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1/","content":"在之前的学习中，我们总是假设概率密度函数的参数形式已知，并在此条件下处理有监督学习过程。而在现实世界中，我们给出的概率密度函数很少符合实际情况，本节我们讨论**非参数化方法(non-parametric method)**，它可以处理任意的概率分布而不必假设密度的参数形式已知。大体上还是遵循着贝叶斯决策论，主要有两个非参数估计的方向：\n\n从训练样本中估计类条件概率密度：\n直接估计后验概率：\n\n[TOC]\n一 概率密度的估计估计类条件概率密度的最基本的一个条件就是：一个向量x落在区域中的概率为是在区域上关于的导数的积分（相当于区域R内每个点的概率密度函数的积分，概率密度函数的积分就是概率函数），因此概率是概率密度函数的平滑（或取平均）的版本，因此我们可以通过概率来估计概率密度函数。\n先假设是连续的，并且区域足够小，以至于在这个区间中条件概率密度几乎没有变化，若其中表示区域所包含的体积（二维面积，三维代表体积）有：假设n个样本都是根据概率密度函数独立同分布(i,i,d)的抽取而得到的，其中有k个样本落在区域中的概率服从二项式定理：当样本足够大时，综合(2)式与(3)式，我们能够得到的估计为：让代表区域中样本的确切数量，有：\n\n推导过程如下图所示：\n\n\n类比利用频率直方图估计概率密度函数，理论上当我们的样本足够多，同时频率分布直方图组距设置的特别小，就是在逼近样本点的真实概率密度函数【2】：\n\n\n\n\n为了估计点处的概率密度函数，构造了一系列的包含的区域，其中第一个区域使用一个样本，第二个区域使用2个样本….，记为区域的体积，为落在中的样本的个数，而表示对的第n次估计：要求估计的概率密度函数收敛到真实值：必须满足以下三个条件，以及他们分别代表的意义【3】：\n\n\n\n随着样本数量的增加，体积尽可能小，类比频率分布直方图的组距尽可能小\n在小区域内有足够多的样本，保证频率之比能够收敛到概率\n在小区域内的样本数在总样本中所占的比例是很小的一部分\n\n\n有两种经常采用的估计途径：\n\nParzen窗：根据某一个确定的体积函数，比如，来逐渐收缩一个给定的初始区间。（要求，能保证能收敛到）\nK-nearest-neighbor：确定为n的某个函数，比如，这样体积必须逐渐生长，直到最后能包进的个相邻点。\n\n这两种方法最终都能收敛到真实概率，但在有限样本下效果不好\n\n\n二 Parzen窗方法2.1 原理假设区间是一个d维的超立方体，如果表示超立方体一条边的长度，则体积为：，通过定义最简单的方型窗函数，得到训练集中的点是否落在落在窗中：\n$$\\varphi(\\frac{\\textbf{x}-\\textbf{x}’}{h_n})=\\left{其他\\right.$$该窗函数定义了一个d维空间中，中心点在点的超立方体。因此代表超立方体中的样本个数是：\n\n\n代入公式（11）得到Parzen窗概率密度函数：\n\n\n窗口函数本质上是出现在该区域内部的采样点的加权频数，可以看做是一种平滑。更一般的，窗函数并不一定是超立方体定义的函数，它可以是任意形式只要保证：\n\n\n\n另一个常见的高斯窗口函数如下：\n\n\n高斯窗口函数，随着采样点与中心点距离的增大而减小。\n当采用高斯窗口函数时，是出现在该区域内部的采样点的加权频数，每个采样点的权重取决于它们与中心的距离。\n\n回过头来看Parzen窗估计的概率密度函数（x代表测试样本，xi代表训练集数据）：表示我们对的估计是对一系列关于和的函数做平均，在本质上，是每一个样本依据它离的远近不同而对结果做出不同贡献。而且在进行估计前要确定一个具体形式的函数\n2.2 窗宽的影响我们定义如下，可以重写：\n\n\n因为，窗宽会显著影响的振幅与宽度\n\n\n\n\n\n如果非常大，那么的影响就很低，即使距离很远，和的差别也不大，这种情况下，是n个宽的、满变的函数的叠加，因此非常平滑\n\n如果很小，的峰值就非常大，这种情况下，是n个以样本为中心的尖脉冲的叠加，也就是一个充满噪声（不确定性）的估计\n\n\n\n\n\n的收敛性： 是样本点（随机变量）的函数， 所以我们希望随机变量的均值和方差满足：\n\n\n经过证明需要满足以下条件，证明过程可参考【4】：\n\n\n这就告诉我们对于的选择，当n趋向于正无穷时趋近于0，但必须以低于1/n的速率，因此常设定或\n\n总结：对于窗宽（或）的选取在很大程度上影响。如果太大，那么估计结果的分辨率就太低，如果太小，那么估计结果的统计稳定性就不够。当样本有限时，尽可能的取折中的估计，然而当样本个数无限，那么就可以在n增加时，让缓缓趋近于0，这样就收敛到某个概率密度函数\n下面举一个例子说明窗宽对估计结果的影响：\n\n\n\n\n下图是一个二维Prazen窗的两类分类器，左图是小窗宽，右图为大窗宽\n\n\n三 K-Nearest Neighbor方法在Parzen窗中，我们固定了体积求解，但最佳的窗函数的选择总是一个问题。另一种思路是固定，让体积进行变化，而不是硬性的规定窗函数为落在区域内的全体样本个数的某个函数。比如我们以样本点为中心，让体积扩张，直到包含个样本为止（是关于n的某个特定函数）。这些样本称为点的个最近邻，带入（一）中的非参数估计公式：\n四 后验概率的估计假设我们把一个体积放在点周围，并且能够包含进k个样本，其中个属于类别。对于联合概率密度的估计为：这样对后验概率的估计就是：这里的值可以由Parzen窗与K近邻决定。\n\nParzen窗方法中，必须是关于n的某个固定形式的函数，比如或\nKn最近邻方法中，必须保证能够包含进足够的样本个数，比如\n\n如果有足够多的样本点并且体积足够小就可以证明这样处理是比较准确的。\n五 最近邻规则最近邻可以看作的k=1情况下的K近邻方法，最近邻规则描述了我们只依赖某个的单一的最近的邻居来做估计，也能达到足够好的性能：令，每个样本已标记，对于测试样本点，在集合D中距离它最近的点的类别为样本点的类别。\n最近邻规则是次优的方法，通常的误差率比贝叶斯误差要高，然而在无限训练样本的情况下，这个误差率至多不会超过贝叶斯误差率的两倍。\n通过最近邻规则，我们可以把样本特征空间分为一个个小的单元格(cell)，每个单元格中的点到最近邻的距离都比到别的样本点的距离要小。这个小单元格中的任意点的类别就与最近邻的类别相同。被称为：空间Voronoi网格。\n\n\n将相同类型的网格的边界相连就可以形成决策边界，最近邻的边界通常是不平滑的，我们可以通过扩大K值来去除噪音，平滑边界。\n\n\n参考【1】模式识别\n【2】非参数估计_音程的博客-CSDN博客_非参数估计\n【3】非参数估计 - 简书 (jianshu.com)\n【4】经典的非参估计 （二）parzen 窗 - 知乎 (zhihu.com)\n","categories":["机器学习"],"tags":["机器学习","非参数估计"]}]