[{"title":"最大似然估计与贝叶斯估计","url":"/2022/01/10/%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1%E4%B8%8E%E8%B4%9D%E5%8F%B6%E6%96%AF%E4%BC%B0%E8%AE%A1/","content":"参数估计在贝叶斯决策论中，我们已经学习了如何根据先验概率与类条件概率密度来设计最优分类器。但在实际应用中，通常得不到有关问题的概率结构的全部信息。通常的解决方案是利用这些训练样本来估计问题中所涉及的先验概率和类条件密度函数，并把这些估计的结果当做实际问题的先验概率和类条件概率密度，然后在设计分类器\n在典型的监督学习问题中，有标注的样本估计先验概率不困难，最大的困难在于估计类条件概率密度：\n\n已有的训练样本数太少，很难满足所有的特征都存在的情况\n当用于表示特征的向量x的维数较大时，就会产生严重的计算复杂度问题（算法的执行时间，系统的资源开销..）\n\n但如果先验知识允许我们把条件概率密度进行参数化，例如：我们可以假设是一个多元高斯分布，其均值是，协方差矩阵为。这样我们就把问题从估计完全未知的类条件概率密度转化为了估计参数与。这样的方法被称为参数估计方法。与之对应的也有非参数估计方法。\n参数估计问题是统计学中的经典问题，主要的解决方案有两种，分别对应统计学中的两大学派：\n\n最大似然估计——频率学派（Frequentist）\n贝叶斯估计——贝叶斯学派（Bayesian）\n\n当然，在参数估计完成后，我们仍然使用后验概率作为分类准则。\n最大似然估计参数分量根据每个样本所属的类别对样本集进行分类：，任意一个分类样本集中的样本都是独立的根据类条件概率密度函数来抽取的。因此获得一个重要假设：**每个样本集中的样本都是独立同分布的随机变量(independent and identically distributed：i.i.d)**，我们还假设每一个类的类条件概率密度的形式都是已知的，未知的是具体的参数向量的值。比如：假设服从多维正态分布，这样向量就由分量所组成。\n为了强调类条件概率密度函数依赖于参数向量的情况，通常写为形如的形式。因此最大似然估计（MLE）解决的就是正确的估计各个类别的具体的参数向量：。\n基本原理问题其实就变成了一个条件概率最大的求解，即在已有的训练集的条件下，求使得 最大的参数，形式化表达为求解而根据条件概率公式有因为我们在极大似然估计中假设是确定的，所以就是一个常数。同样是根据已有的数据得到的，也是确定的，或者我们可以把其看作是对整个概率的一个归一化因子。这时候，求解公式 (1) 就变成了求解一个有监督的总的样本集分为c类，我们要估计每个类的后验概率，有c个独立的问题，每个问题解决思路相同如下：已知样本集中有n个样本。由于这些样本独立同分布，因此由（3）中的似然函数得：【2】因为样本集已知，可以看作是参数向量的函数，**参数向量的最大似然估计，就是使似然函数：达到最大值的参数向量，记为**。为了方便分析取对数，定义对数似然函数：\n如果实际的待求参数的个数为p，则参数向量可以写成如下的p维向量的形式：。记为梯度算子（构建求偏导数的数学式）这样用数学语言表示求的全过程：\n\n\n注意：求得的解可能是全局最大值点，也可能是局部极值点。\n高斯情况深入讨论当训练样本服从多元正态分布时的情况。\n协方差矩阵已知，而均值未知\n\n\n\n我们得到以下结论，对于均值的最大似然估计就是对全体样本取平均。\n协方差矩阵和均值都未知高斯分布的更典型的情况是协方差矩阵和均值都未知。这样参数向量由两个分量组成。\n\n先考虑单变量的情况\n  \n\n  \n当高斯函数是多元时，最大似然估计的结果是：  \n  均值的最大似然估计就是样本的均值，而协方差的最大似然估计是n个的算术平均。实际上对方差的最大似然估计是有偏的估计，样本的协方差矩阵，而我们估计的方差是\n\n\n&gt; 样本协方差矩阵的推导过程【2】：\n&gt;\n&gt; {% asset_img dff0de718d34e83e9fd3f6c931675889_720w-16418259715585.jpg dff0de718d34e83e9fd3f6c931675889_720w %}\n\n贝叶斯估计基本原理\nMLP通过最大化似然函数从而找出参数，思想在于找出能最大概率生成这批数据的参数。但是这种做法完全依赖于数据本身，当数据量大的时候，最大似然估计往往能很好的估计出参数 ；但是当数据量小的时候，估计出来的结果并不会很好。就例如丢硬币问题，当我们投掷了5次，得出了正正正正正的结果，极大似然估计会得出投出正面的概率为100%！这显然是不符常理的。\n贝叶斯派的人认为，后验概率中被估计的参数同样服从一种已知的分布，即参数也为一个随机变量。他们在估计参数前会先带来先验知识，例如参数在[0.5,0.6]的区域内出现的概率最大，在引入了先验知识后利用样本估计出参数分布的形式，在数据量小的情况下估计出来的结果往往会更合理。【2】\n\n我们希望利用现有的全部信息来为测试样本x计算分类的依据：后验概率（现有的全部信息一部分为我们的先验知识，比如未知概率密度函数的形式，未知参数取值范围；另一部分则来自训练集本身），假设已有训练集，改写后验概率为：表示训练集的重要性，则贝叶斯公式为：\n\n这一公式指出我们能够根据训练样本提供的信息来确定类条件概率密度和先验概率。\n\n通常认为先验概率可以事先得到，或简单计算得到，可以简写为\n\n有监督学习可以把每个样本都归到它所属的类中，如果，那么样本集中的训练样本就对没有任何影响，这样就产生两个简化：\n\n能够对每一个类分别处理：只是用中的训练样本就能确定\n能够对每个类进行分别处理，公式中说明类别的符号都可以省略\n\n  本质上我们处理的是c个独立的问题，每个问题形式：已知一组训练样本D，根据这些样本估计（未简化为）\n\n\n\n参数估计法的前提是每个类的类条件概率密度形式已知，参数向量未知。对于任意一个测试样本x，去除类符号简化为且形式已知。而贝叶斯估计与最大似然估计的区别体现在，我们不认为是一个固定但未知的值，而是服从一个概率分布。根据、、与，我们可以得到未知参数的后验概率密度函数其中【这里是训练样本集的某一个样本】接着根据积分估算测试样本的后验概率：由于对测试样本x和训练样本集D的选取是独立进行的，，公式(10)改为：\n总结整个贝叶斯估计分为三个阶段：\n\n根据样本与先验知识求得参数的后验概率\n根据参数的后验概率与类条件概率密度的形式求得每个类的具体的类条件概率密度\n最后求出每个类的后验概率\n\n\n\n\n高斯情况下的贝叶斯估计对高斯正态分布的情况，用贝叶斯估计的方法来计算未知参数的后验概率密度函数和设计分类器所需的类条件概率密度函数，其中假设（相当于第一步中的）\n单变量情况第一步：先考虑只有均值未知的情况，同时假设特征向量是一维的。，而且我们认为所有关于均值的先验知识都在先验概率密度函数中，且均值服从（已知），根据3.1节总结的贝叶斯估计三步走：\n\n\n\n\n我们发现是一个指数函数，且指数部分是的二次型，因此实际上任然是正态分布，把写成下面的形式：\n\n\n利用对应位置系数相等的原则就可以求出的值：\n\n\n上述方程显示了先验知识和样本观测结果是如何被结合在一起，并且形成的后验概率密度函数的，总的来说：\n\n代表在观测n个样本后对的真实值的最好估计，是与的线性组合\n\n表示对这种估计的不确定性（取值更离散，不集中）\n\n如果，当n趋近于，趋近于样本均值，\n当时，，这是一种退化的情况：我们对先验估计是如此的确信，以至于任何观测样本都无法改变我们的认知。\n当远大于，表示我们对先验估计如此的不确信以至于直接拿样本均值当做\n\n  根据公式（7），是n的单调递减函数，也就是说每观测一个样本，我们对的估计的不确定性就会减小，当增加时，的概率密度图就会变得更尖锐，当n趋近于无穷大时，逼近狄拉克函数。\n\n\n\n狄拉克δ函数是一个广义函数，在物理学中常用其表示质点、点电荷等理想模型的密度分布，该函数在除了零以外的点取值都等于零，而其在整个定义域上的积分等于1。\n狄拉克δ函数在概念上，它是这么一个“函数”：在除了零以外的点函数值都等于零，而其在整个定义域上的积分等于1。【4】\n\n\n\n单变量情况第二步：在得到均值的后验概率密度后，我们就可以计算类条件概率密度（实际上是，把c个问题独立，去除类有关变量后）\n\n\n\n\n求得类条件概率密度后再带入到公式(19)就能求出后验概率了。\n最大似然估计与贝叶斯估计比较最大似然估计与贝叶斯估计在训练样本趋于无穷时效果是一样的，然而在实际的问题中，训练样本总是有限的，决定我们选择哪个方法的主要因素有\n\n计算复杂度：MLE只涉及一些微分与求极值，而贝叶斯估计计算复杂的积分\n可理解性：MLE是基于设计者所提供的训练样本的一个最佳解答，而贝叶斯估计方法得到的结果则是许多可行解的加权平均值，反映出对各种可行解的不确定程度\n对初始先验知识的信任程度：MLE得到的类条件概率密度形式与先验的假设是一样的，而贝叶斯估计得到的形式可能与初始假设不相同，贝叶斯方法能比MLE利用更多有用的信息。\n\n总结贝叶斯方法有很强的理论基础，但在实际应用中最大似然估计更加简便，且性能也相差无几。\n参考【1】模式分类（第二版）\n【2】(1条消息) 极大似然估计与贝叶斯估计_Jim_Liu-CSDN博客_极大似然估计和贝叶斯估计\n【3】(20 封私信 / 29 条消息) 为什么样本方差（sample variance）的分母是 n-1？ - 知乎 (zhihu.com)\n【4】百度百科：狄拉克δ函数\n","categories":["机器学习"],"tags":["机器学习"]},{"title":"正态密度下的分类器","url":"/2022/01/10/%E6%AD%A3%E6%80%81%E5%AF%86%E5%BA%A6%E4%B8%8B%E7%9A%84%E5%88%86%E7%B1%BB%E5%99%A8/","content":"一个贝叶斯分类器可由条件概率密度p(x|ωi)和先验概率P(ωi)决定。在各种密度函数中，高斯密度函数（多元正态函数）最受青睐。本节我们先从单变量高斯密度函数谈起，接着探讨多元高斯分布以及一些特殊情况下的判别函数。\n单变量高斯密度函数单变量正态或高斯密度函数，变量x遵循x~N(μ,σ^2)，其概率密度函数为：因此可以求出x的期望与方差：\n\n\n\n\n\n\n\n\n\n如中心极限定理所表示，大量的小的、独立的随机分布的总和等效为一个高斯分布，对于实际的概率分布而言高斯分布是一种很好的模型\n\n多元密度函数一般的d维多元正态分布密度及其相关统计量形式如下：\n\n\n其中x是一个d维列向量，μ是x的d维均值向量，Σ 是d*d的协方差矩阵，这里的(x-μ)(x-μ)T是向量的内积。均值向量与协方差矩阵的分量形式可写为：$$\\mu_i = E[\\textbf{x}i]\\\\sigma{ij}=E[(x_i-\\mu_i)(x_j-\\mu_j)]$$多元高斯分布的协方差矩阵有以下性质：\n\n协方差矩阵是对称且半正定的\n\n协方差矩阵的对角线元素表示各维的方差，非对角线元素表明两维之间的协方差。\n\n对于高斯分布来说，独立等价于不相关，所以如果xi与xj统计独立，则。\n\n\n服从正态分布的随机变量的线性组合，不管这些随机变量是独立的还是非独立的，线性组合也是正态分布。多元高斯分布有线性不变性：\n\n\n正态分布下的判别函数我们之前通过后验概率构造的判别函数g(x)：如果类条件概率密度函数p(x|ωi)是多元正态分布N(μi,Σi)，带入表达式可以化简为：其中最后一项与x无关，实际计算过程中可以省略。我们讨论一些特殊情况下的判别函数以及分类结果。\n这种情况发生在各特征统计独立，并且每个特征的具有相同的方差时。这种情况下所有类型的协方差矩阵相同，都是对角矩阵且为单位矩阵I与方差的乘积。因此 ，因此(6)式可以化简为：\n\n\n，继续观察。\n\n\n一个线性分类器的判定面是一些超平面，这些超平面是由线性方程来确定的，以上的例子中，此方程可以写成：\n\n\n继续变换：\n\n\n由于，特征空间中属于i类的类别空间与属于j类的类别空间分开的超平面与两个空间的中心点的连线垂直，当所有类别的先验概率相等时，就是中心点。\n这种情况下，最优判决规则从计算g(x)跟直观的改为——最小距离分类器：为了将某一特征向量x归类，通过测量每一个x到c个均值向量中的每一个欧氏距离（二维平面内的距离），并将x归为离他最近的那一类中。\n\n下图为先验概率相等的情况下的例子：\n  \n当先验概率不相等时判决边界可能出现偏移：\n  \n\n第二种情况是所有类的协方差矩阵都相等，但各自的均值向量是任意的，则由式(6)可得\n\n\n由于判别函数是线性的，判决边界同样是超平面，同3.1 计算与的边界\n\n\n由于并非朝着的方向，因而分离与的超平面也并非与均值间的连线垂直正交，但如果先验概率相等，x0还是均值向量的中心点。\n任意一般情况下，每一类的协方差矩阵都是不同的，式(6)中唯一可以去掉的只有(d/2)ln2π，\n\n\n在两类问题中，判定面是超二次曲面，甚至在一维情况下，其判决区域可以不连通。\n\n\n\n\n\n\n\n\n例：二维高斯分布数据的判决区域\n尝试计算上图的贝叶斯判别边界。以表示红点的集合，表示红点的集合。在这里我们假设只需要计算均值与方差，利用离散随机变量的均值与方差的定义可得。以的计算为例：\n\n\n因此：因为与不相同，与方差也不相同，属于第三类：任意。假设两类分布的先验概率相等（）带入到3.3节的公式中，则的判别边界如图中的顶点是（3 , 1.83）二次曲线，为:尽管两种分布的数据沿方向的方差相等（协方差矩阵的第二行），但判别边界并不通过两均值向量（[3,6]；[3,2]）的中点。这是因为对于分布而言，沿方向的概率分布相比与分布受到挤压（样本沿分布的更宽，且协方差矩阵第一行更大），由于总的先验概率相等（整个特征空间的积分【面积】相等），那么沿方向的分布将要增加（相对于），因此判别边界位于两均值向量的中点偏方向。\n参考【1】模式分类（第二版）\n","categories":["机器学习"],"tags":["机器学习"]},{"title":"非参数估计","url":"/2022/01/10/%E9%9D%9E%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1/","content":"在之前的学习中，我们总是假设概率密度函数的参数形式已知，并在此条件下处理有监督学习过程。而在现实世界中，我们给出的概率密度函数很少符合实际情况，本节我们讨论**非参数化方法(non-parametric method)**，它可以处理任意的概率分布而不必假设密度的参数形式已知。大体上还是遵循着贝叶斯决策论，主要有两个非参数估计的方向：\n\n从训练样本中估计类条件概率密度：\n直接估计后验概率：\n\n概率密度的估计估计类条件概率密度的最基本的一个条件就是：一个向量x落在区域中的概率为是在区域上关于的导数的积分（相当于区域R内每个点的概率密度函数的积分，概率密度函数的积分就是概率函数），因此概率是概率密度函数的平滑（或取平均）的版本，因此我们可以通过概率来估计概率密度函数。\n先假设是连续的，并且区域足够小，以至于在这个区间中条件概率密度几乎没有变化，若其中表示区域所包含的体积（二维面积，三维代表体积）有：假设n个样本都是根据概率密度函数独立同分布(i,i,d)的抽取而得到的，其中有k个样本落在区域中的概率服从二项式定理：当样本足够大时，综合(2)式与(3)式，我们能够得到的估计为：让代表区域中样本的确切数量，有：\n\n推导过程如下图所示：\n\n\n类比利用频率直方图估计概率密度函数，理论上当我们的样本足够多，同时频率分布直方图组距设置的特别小，就是在逼近样本点的真实概率密度函数【2】：\n\n\n\n\n为了估计点处的概率密度函数，构造了一系列的包含的区域，其中第一个区域使用一个样本，第二个区域使用2个样本….，记为区域的体积，为落在中的样本的个数，而表示对的第n次估计：要求估计的概率密度函数收敛到真实值：必须满足以下三个条件，以及他们分别代表的意义【3】：\n\n\n\n随着样本数量的增加，体积尽可能小，类比频率分布直方图的组距尽可能小\n在小区域内有足够多的样本，保证频率之比能够收敛到概率\n在小区域内的样本数在总样本中所占的比例是很小的一部分\n\n\n有两种经常采用的估计途径：\n\nParzen窗：根据某一个确定的体积函数，比如，来逐渐收缩一个给定的初始区间。（要求，能保证能收敛到）\nK-nearest-neighbor：确定为n的某个函数，比如，这样体积必须逐渐生长，直到最后能包进的个相邻点。\n\n这两种方法最终都能收敛到真实概率，但在有限样本下效果不好\n\n\nParzen窗方法原理假设区间是一个d维的超立方体，如果表示超立方体一条边的长度，则体积为：，通过定义最简单的方型窗函数，得到训练集中的点是否落在落在窗中：\n'_' allowed only in math mode \\varphi(\\frac{\\textbf{x}-\\textbf{x}’}{h_n})=\\begin{cases} 1 &amp; \\text{|x_j-x_j’|\\leq h_n/2;j=1,…d}\\ 0 &amp; \\text{其他} \\end{cases} 该窗函数定义了一个d维空间中，中心点在点的超立方体。因此代表超立方体中的样本个数是：\n\n\n代入公式（11）得到Parzen窗概率密度函数：\n\n\n窗口函数本质上是出现在该区域内部的采样点的加权频数，可以看做是一种平滑。更一般的，窗函数并不一定是超立方体定义的函数，它可以是任意形式只要保证：\n\n\n\n另一个常见的高斯窗口函数如下：\n\n\n高斯窗口函数，随着采样点与中心点距离的增大而减小。\n当采用高斯窗口函数时，是出现在该区域内部的采样点的加权频数，每个采样点的权重取决于它们与中心的距离。\n\n回过头来看Parzen窗估计的概率密度函数（x代表测试样本，xi代表训练集数据）：表示我们对的估计是对一系列关于和的函数做平均，在本质上，是每一个样本依据它离的远近不同而对结果做出不同贡献。而且在进行估计前要确定一个具体形式的函数\n窗宽的影响我们定义如下，可以重写：\n\n\n因为，窗宽会显著影响的振幅与宽度\n\n\n\n\n\n如果非常大，那么的影响就很低，即使距离很远，和的差别也不大，这种情况下，是n个宽的、满变的函数的叠加，因此非常平滑\n\n如果很小，的峰值就非常大，这种情况下，是n个以样本为中心的尖脉冲的叠加，也就是一个充满噪声（不确定性）的估计\n\n\n\n\n\n的收敛性： 是样本点（随机变量）的函数， 所以我们希望随机变量的均值和方差满足：\n\n\n经过证明需要满足以下条件，证明过程可参考【4】：\n\n\n这就告诉我们对于的选择，当n趋向于正无穷时趋近于0，但必须以低于1/n的速率，因此常设定或\n\n总结：对于窗宽（或）的选取在很大程度上影响。如果太大，那么估计结果的分辨率就太低，如果太小，那么估计结果的统计稳定性就不够。当样本有限时，尽可能的取折中的估计，然而当样本个数无限，那么就可以在n增加时，让缓缓趋近于0，这样就收敛到某个概率密度函数\n下面举一个例子说明窗宽对估计结果的影响：\n\n\ns\n\n\n下图是一个二维Prazen窗的两类分类器，左图是小窗宽，右图为大窗宽\n\n\nK-Nearest Neighbor方法在Parzen窗中，我们固定了体积求解，但最佳的窗函数的选择总是一个问题。另一种思路是固定，让体积进行变化，而不是硬性的规定窗函数为落在区域内的全体样本个数的某个函数。比如我们以样本点为中心，让体积扩张，直到包含个样本为止（是关于n的某个特定函数）。这些样本称为点的个最近邻，带入（一）中的非参数估计公式：\n\n\n后验概率的估计假设我们把一个体积放在点周围，并且能够包含进k个样本，其中个属于类别。对于联合概率密度的估计为：这样对后验概率的估计就是：这里的值可以由Parzen窗与K近邻决定。\n\nParzen窗方法中，必须是关于n的某个固定形式的函数，比如或\nKn最近邻方法中，必须保证能够包含进足够的样本个数，比如\n\n如果有足够多的样本点并且体积足够小就可以证明这样处理是比较准确的。\n最近邻规则最近邻可以看作的k=1情况下的K近邻方法，最近邻规则描述了我们只依赖某个的单一的最近的邻居来做估计，也能达到足够好的性能：令，每个样本已标记，对于测试样本点，在集合D中距离它最近的点的类别为样本点的类别。\n最近邻规则是次优的方法，通常的误差率比贝叶斯误差要高，然而在无限训练样本的情况下，这个误差率至多不会超过贝叶斯误差率的两倍。\n通过最近邻规则，我们可以把样本特征空间分为一个个小的单元格(cell)，每个单元格中的点到最近邻的距离都比到别的样本点的距离要小。这个小单元格中的任意点的类别就与最近邻的类别相同。被称为：空间Voronoi网格。\n\n\n将相同类型的网格的边界相连就可以形成决策边界，最近邻的边界通常是不平滑的，我们可以通过扩大K值来去除噪音，平滑边界。\n\n\n参考【1】模式识别\n【2】非参数估计_音程的博客-CSDN博客_非参数估计\n【3】非参数估计 - 简书 (jianshu.com)\n【4】经典的非参估计 （二）parzen 窗 - 知乎 (zhihu.com)\n","categories":["机器学习"],"tags":["机器学习"]},{"title":"神经网络","url":"/2022/01/10/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/","content":"生物神经网络到人工神经网络人工神经网络的构筑理念是受到生物（人或其他动物）神经网络功能的运作启发而产生的。神经元大致可以分为：树突、突触、细胞体和轴突。\n在生物神经网络中，每个神经元与其他神经元相连，当它兴奋时，就会向相连的神经元发送化学物质，从而改变这些神经元的电位；如果某神经元的电位超过了一个阈值，它就会被激活，即”兴奋”起来，向其他神经元发送化学物质。\n\n\n人工神经网络（artificial neural network，ANN），简称神经网络（neural network，NN），是一种模仿生物神经网络的结构和功能的数学模型或计算模型。神经网络由大量的人工神经元联结进行计算，【2】是：\n\n模拟大脑学习过程的计算模型\n具有神经元的基本特征以及类似大脑的神经元的连接。\n\n神经网络是由简单处理单元组成的大规模并行分布式处理器，具有存储实验知识并使其可用的自然倾向。它在两个方面与大脑相似：\n\n知识的获取方式是网络通过一个学习的过程在环境中获取的\n神经元间连接强度被称为突触权重，用来储存获得的知识\n在学习过程中，为了在训练中正确地为特定的学习任务建模，需要对权值进行了修改\n\n神经网络的发展历史：\n\nFrank Rosenblatt，（康奈尔大学的心理学家）在1958年，在《 纽约时报 (New York Times)》上发表文 章《Electronic ‘Brain’ Teaches Itself.》，正式把算 法取名为“感知器”。\n  它有400个光传感器，它们一起充当视网膜，将 信息传递给大约1000个“神经元” ，这些神经元进行处理并输出单一信息。\n  \n马文·明斯基（人工智能之父” (Marvin Minsky) 1970图灵奖获得者）1969年，Minsky 和Papert所著的《Perceptron》一书出版， 从数学角度证明了关于单层感知器的计算具有根本的局限性， 指出感知器的处理能力有限，甚至连XOR这样的问题也不能解决。神经网络进入了萧条期\n\n杰弗里·辛顿（“神经网络之父”(Geoffrey Hinton) 2019图灵奖获得者）1986年在多伦多大学的辛顿实现了一种叫做反向传播的 原理来让神经网络从他们的错误中学习\n\n\n单层感知机网络感知机模型感知机（Perceptron）或神经元（Neuron）是神经网络中最基本的信息处理单元，它模拟生物神经元，由以下几个部分组成：\n\n输入一组特征向量：\n\n一组突触或连接，每根连接都会赋予一个权重：\n\n一个偏置(bias)：，偏置增加了感知机的灵活性\n\n一个加法器函数(线性组合器)，计算通过突触输入的加权值的和：\n\n激活函数(压实函数)，用于限制神经元输出的振幅：\n\n\n\n\n为了保持加权和的形式统一我们可以添加一个输入，权值，形成新的输入特征向量在特征增广空间中穿过原点。\n\n\n激活函数\n\n\n\n\n\n\n\n感知机分类图示假设我们有训练集分为两种类型：和。其中，样本表示为偏置，假设T1与T2是线性可分的（linearly separable），能否给出一个感知机将数据正确划分？\n\n\n我们使用单层感知机模型来解决这个问题，激活函数选择符号函数。输出1为1类正例，输出-1为2类负例。\n\n\n假设数据的维数是二维，有个测试数据真实标签为1类，下图的这个向量能够正确的分类，因为计算激活函数的值为1。\n\n\n但下图的却不能正确分类\n\n\n如何更新w让感知机变得可行？应该让加上一个使得\n\n\n对于分类结果的真实类别与预测类别一共有以下四种情况：\n\n\n\n真实类别\n预测类别\n差\n\n\n\n1\n-1\n2\n\n\n-1\n1\n-2\n\n\n1\n1\n0\n\n\n-1\n-1\n0\n\n\n因此我们设计一种误差：\n\n，把正例错分成负例，假负例\n，把负例错分成正例，假正例\n，预测正确\n\n再配上**学习率**，来构造一个标准化，用于更新激活函数中的\n感知机的学习策略对于2.3节的问题，假设我们的误差函数是均方误差的形式：\n\n\n在m=2时，也就是每个数据有两个特征时，可以画出近似的误差函数图像：\n\n\n我们的目标是减小误差，直观的来说就是“下山”，因此学习策略采用梯度下降法：\n\n对于任意一个函数，定义一个梯度算子，。\n则f的全导数\n梯度(向量)表示某一函数在该点处的方向导数沿着该方向取得最大值，即函数在该点处沿着该方向（此梯度的方向）变化最快，变化率最大（为该梯度的模）。\n\n为了最小化误差函数的值，我们选择向负梯度的方向学习\n\n\n\n\n\n\n多层神经网络\n\n上图显示的是一个简单的异或(XOR)问题三层神经网络，这个网络由一个输入层，一个隐含层（它的输入与输出并不为外部环境直接所见）和一个输出层组成。它们由可修正的权值相连，除了连接输入单元，每个单元还连接者一个**偏置(bias)**，单元也被称为神经元（neuron），我们要利用这种网络来做模式识别，输入单元提供特征量，输出单元激发的信号成为用来分类的判别函数的值。\n通过一个最简单的非线性问题了解多层神经网络——异或(XOR)问题：给定两个特征(x1,x2)，输出他们异或的值。\n每个二维输入向量都提供给输入层，每个输入神经元的输出结果就是对应的分量，隐含层单元对它的各个输入进行加权求和运算而形成标量的净激活（net activation简称net），为了简单，我们增广输入向量(附加一个特征值)和权值()，因此其形式定义：下标i是输入层单元的索引，j是输出层单元的索引，表示输入层第i个神经元到隐含层第j个神经元的权值。每个隐含层单元激发出一个输出分量，这个分量是将净激活传入一个激活函数中，该问题中定义激活函数为符号函数(sign)：每个输出单元同隐含层单元计算它的净激活：这样最后的输出，我们用这个输出来解决异或问题。容易验证，上述给定权值的三层网络的确可以解决异或（XOR）问题。它计算判决边界为 x1+x2+0.5 = 0。另一个隐含层单元的判别边界x1+x2-1.5=0。只有y1与y2都等于1，最终z才激发为+1。\n反向传播学习算法(Back Propagation)对于一个三层的神经网络，可以很直接的根据其误差，找到隐含层与输出层的权值更新，这与线性LMS算法类型，可是如何训练从输入层到隐函层的权值呢？如果一个隐含层单元的“适当”输出对每种模式都是已知的，那么输入层到隐含层的权值就可以调节到很接近它。BP算法允许我们对每个隐含层单元计算有效误差，并由此推导出一个输入层到隐含层权值的学习规则\n神经网络中有两种基本的学习模式：前馈与反向传播。\n\n前馈：在网络间传递信号，然后在输出层得到输出。\n反向传播：对于一个有监督学习，通过修改网络中每个节点的连接权重来使实际输出更接近真实值。\n\n\n\n对于一个d-n-c的全连接三层网络：\n\n一个d维的输入向量被提供给输入层，每个输入单元发送它对应的分量\n\nn个隐含层单元的每个都计算它的净激活能，他是输入层和隐含层单元权值的内积\n\n隐含层单元的输出是，f是一个激活函数，这里选择Sigmod函数。\n\n输出层单元类似于隐含层，计算净激活能，网络的最终信号\n\n是学习率。\n\n\n从输出层到隐含层根据期望输出值与实际输出值，我们先定义一个误差函数(LossFunction)：反向传播学习规则是基于梯度下降法的。权值向量首先被初始化为随机值，然后向误差减小的方向调整：\n针对上图的三层网络，考虑一个隐含层神经元到一个输出层神经元的权值分量，利用链式求导法则：其中单元k的敏感度定义为误差关于输出层净激活能的偏导，而误差又与激活函数的值直接相关：（8）式的最后一项由，对于具体的一个隐含层神经元与该神经元到输出层的权值因此一个隐含层神经元()到一个输出层神经元()的权值()更新规则：\n从隐含层到输入层(隐含层)从输入层到隐含层的权值学习规则不太一样，对误差求关于任意一个输入层到隐含层权值()的偏导：而对于第一项，因为从隐含层到输出层是全连接，一个具体的输出输入到了所有的输出层中，要对所有的输出层误差求和再偏导：、类似的定义一个隐含层单元的敏感度为各输出单元的敏感度的加权和，权重为隐含层到输出层的权重，因此输入层到隐含层的学习策略是：（9）式与（16）式共同给出了反向传播算法，确切的说是误差反向传播算法。\n\n\n\n\n简单的反向传播例子下面通过一个简单三层神经元的例子来直观的感受BP算法的过程，b代表偏置，激活函数选择sigmod函数：\n\n\n\n计算h1的输入：$h_1=w_1i_1+w_2i_2+b_1*1=0.3775$\n\n计算h1的输出值：\n\n同理计算h2的输出值y2 = 0.596884378。\n\no1的输入值：$o_1=w_5y_1+w_6y_2+b_2*1=1.105905967$\n  o1的输出值为sigmod(1.105905867)=0.75136507\n\n同理o2的输出值为0.772928465\n\n\n定义Loss function(损失函数)为平方误差，计算总误差：\n\n\n求总误差关于的梯度：\n\n\n\n\n\n因此总误差关于w5权重的梯度方向为：\n\n\n取学习率为0.5，w5的变化率：\n\n\nBP算法的优化规格化假设我们使用2-输入的网络，利用质量（以克为单位）和长度（以米为单位）特征来对鱼进行分类，这种表示法对于一个神经网络分类器具有严重的不足：质量与长度的度量不在一个数量级上，从而在训练的过程中，网络将更多的根据质量输入来调节权值。我们不希望我们的分类器仅根据某一种做出判断，仅仅因为它们在数值表示上不同。\n为了避免这样的问题，输入模式必须重新进行尺度变换(scaling)，常见的尺度变换有归一化：将所有输入标签全部映射在[0,1]区间上。\n\n\n隐含层单元数输入层与输出层单元数分别由输入向量的维数与类别决定，隐含层单元数，决定了网络的表达能力——从而决定了判别边界的复杂度。如果模式较容易分开或线性可分，那么仅需要较少的隐单元；相反需要更多的隐单元。\n隐含层单元的选择应该适中：\n\n隐含层单元过大：训练误差率可能变得很小，但可能过拟合训练集，使得测试集的误差很大\n隐含层单元过小：网络将不具备足够的自由度以较好的拟合训练数据。\n\n一个经验规则是选取隐单元的个数，使得网络中总权重数大约为n/10（n为训练样本数）\n权值初始化我们不能将权值初始化为0，否则学习过程将不可能开始。相关的知识可以看【3】\n学习率原则上，学习率足够小以保证收敛，那么它的值仅仅决定网络中到达最小误差的速度。\n\n学习率过小：可以保证收敛到最小误差，但训练速度太慢\n学习率过大：可能系统会震荡，可能系统会发散无法收敛。\n\n\n\n参考【1】模式分类第二版\n【2】神经网络详解（基本完成）_大土的博客-CSDN博客_神经网络\n【3】啃一啃神经网络——权重初始化 - 知乎zhihu.com\n","categories":["机器学习"],"tags":["机器学习"]},{"title":"实验：贝叶斯决策论预测贷款是否违约","url":"/2022/01/10/%E5%AE%9E%E9%AA%8C%EF%BC%9A%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%86%B3%E7%AD%96%E8%AE%BA%E9%A2%84%E6%B5%8B%E8%B4%B7%E6%AC%BE%E6%98%AF%E5%90%A6%E8%BF%9D%E7%BA%A6/","content":"实验说明背景信用风险是指银行向用户提供金融服务后，用户不还款的概率。信用风险一直是银行贷款决策中广泛研究的领域。信用风险对银行和金融机构，特别是商业银行来说，起着至关重要的作用，但是一直以来都比较难管理。\n本实验以贷款违约为背景，要求使用贝叶斯决策论的相关知识在训练集上构建模型，在测试集上进行贷款违约预测并计算分类准确度。\n实验数据说明训练数据集train.csv包含9000条数据，测试数据集test.csv包含1000条数据。注意，训练集和测试集中都有缺失值存在。以下是字段说明：\n\n\n\n字段\n描述\n\n\n\nloan_id\n贷款记录唯一标识\n\n\nuser_id\n借款人唯一标识\n\n\ntotal_loan\n贷款数额\n\n\nyear_of_loan\n贷款年份\n\n\ninterest\n当前贷款利率\n\n\nmonthly_payment\n分期付款金额\n\n\nGrade/class\n贷款级别\n\n\nemployment_type\n所在公司类型\n\n\nindustry\n工作领域\n\n\nwork_year\n工作年限\n\n\nhome_exist\n是否有房\n\n\ncensor_status\n审核情况\n\n\nissue_date\n贷款发放的月份\n\n\nuse\n贷款用途类别\n\n\npost_code\n贷款人申请时邮政编码\n\n\nregion\n地区编码\n\n\ndebt_loan_ratio\n债务收入比\n\n\ndel_in_18month\n借款人过去18个月逾期30天以上的违约事件数\n\n\nscoring_low\n借款人在贷款评分中所属的下限范围\n\n\nscoring_high\n借款人在贷款评分中所属的上限范围\n\n\nknown_outstanding_loan\n借款人档案中未结信用额度的数量\n\n\nknown_dero\n贬损公共记录的数量\n\n\npub_dero_bankrup\n公开记录清除的数量\n\n\nrecircle_bal\n信贷周转余额合计\n\n\nrecircle_util\n循环额度利用率\n\n\ninitial_list_status\n贷款的初始列表状态\n\n\napp_type\n是否个人申请\n\n\nearlies_credit_mon\n借款人最早报告的信用额度开立的月份\n\n\ntitle\n借款人提供的贷款名称\n\n\npolicy_code\n公开可用的策略_代码=1新产品不公开可用的策略_代码=2\n\n\nf系列匿名特征\n匿名特征f0-f4，为一些贷款人行为计数特征的处理\n\n\nearly_return\n借款人提前还款次数\n\n\nearly_return_amount\n贷款人提前还款累积金额\n\n\nearly_return_amount_3mon\n近3个月内提前还款金额\n\n\nisDefault\n贷款是否违约（预测标签）\n\n\n实验注意事项\n实验不限制使用何种高级语言，推荐使用python中pandas库处理csv文件。\n\n在进行贝叶斯分类之前重点是对数据进行预处理操作，如，缺失值的填充、将文字表述转为数值型、日期处理格式（处理成“年-月-日”三列属性或者以最早时间为基准计算差值）、无关属性的删除等方面。\n\n数据中存在大量连续值的属性，不能直接计算似然，需要将连续属性离散化。\n\n另外，特别注意零概率问题，贝叶斯算法中如果乘以0的话就会失去意义，需要使用平滑技术。\n\n实验目的是使用贝叶斯处理实际问题，不得使用现成工具包直接进行分类。\n\n实验代码中需要有必要的注释。\n\n\n实验数据分析训练样本是没有预处理的数据，直接用来训练模型是不现实的，首先我们要进行数据的分析。\n贷款记录唯一标识(loan_id)、借款人唯一标识(user_id)属于标志属性，与实际问题没有关联，可以删除。\n贷款年份(year_of_loan)、贷款发放的月份(issue_date)、借款人最早报告的信用额度开立的月份(earlies_credit_mon)、贷款人申请时邮政编码(post_code)、地区编码(region)。这些时间地点属性对贷款人的还款行为没有过多影响，也可以剔除。【1】\n通过对剩下的字段画出频率分布直方图分析：\n\n\n是否个人申请(app.type)、公开可用的策略(policy_code)、匿名特征(f1)属于归一化属性，即一个变量大部分的观测都是相同的特征，那么认为此类特征变量无法显著区分目标变量，可以考虑将其删除。\n而工作领域(industry)、(贷款级别)class、(工作年限)work_year、(所在公司类型)employer_type，这些字段都是属于分类变量，需要进行分类处理。\n同时字段total_loan、interest、monthly_payment、debt_loan_ratio、scoring_low、scoring_high、 recircle_b、recircle_u、early_return_amount_3mon的取值是连续值（小数），因此需要对这些数据进行离散化。特征离散化后，模型会更稳定，降低了模型过拟合的风险。\n对于缺失的数据，可以利用众数填充。\n实验原理主要原理本次实验原理主要为朴素贝叶斯决策论。什么是贝叶斯决策论可以看我的上一篇文章：\n\n(3条消息) 贝叶斯决策论理论_Sunburst7的博客-CSDN博客\n\n如果使用普通的贝叶斯方法计算后验概率时，类条件概率是特征向量上所有特征的联合概率，难以从有限的训练样本中直接得到。为了避开这个障碍，朴素贝叶斯分类器采用了属性条件独立性假设：对已知类别，假设所有属性相互独立，换言之，假设每个属性独立地对分类结果发生影响。基于这个假设，我们的后验概率可以修改为：【2】\n3.2 处理分类数据有时候，根据某种特性而不是数量来度量对象会更有效。我们常常使用这种定性的信息来判断一个观察值的属性，比如按照性别、颜色或者车的品牌这样的类别对其分类。本身没有内在顺序的特征类别称为 nominal。分类的特征总是有某种天然顺序的称为ordinary。【3】\n\n对于内部没有顺序的分类（性别，水果类型）：通常使用one-hot编码\n对于内部有顺序的分类（非常同意、同意、保持中立、反对..）：将 ordinal 分类转换成数值，同时保留其顺序。最常见的方法就是，创建一个字典，将分类的字符串标签映射为一个数字，然后将 其映射在特征上。\n\n本次实验中的class、industry、work_year、employer_type都属于分类特征，我们可以采用将特征值映射到一个数字的方法。\n数据离散化对于连续型的特征，在计算似然时很难找到同类型的数据，这样就会出现类条件概率为0的情况，使得分类器判别出现误差，为了避免这种情况，需要将连续特征离散化。这里用到的技术通常被称为数据分箱：【4】\n\n\n常见的分箱方法主要分为有监督与无监督两种，本实验采用卡方分箱对数据离散化：\n\n无监督分箱：不需要提供预测标签，仅凭借特征就能实现分箱\n等宽分箱\n等频分箱\n\n\n有监督分箱：需要结合预测标签的值，通过算法实现分箱\n决策树分箱\n卡方分箱：关于卡方分箱的原理可以看参考【5】的博客\n\n\n\n拉普拉斯平滑计算要预测数据集的某个特征似然时，如果在观察样本库（训练集）中没有出现过，会导致类条件概率结果是0。在贝叶斯分类中如果乘以0的话，整个后验概率就会失去意义。\n为了解决零概率的问题，法国数学家拉普拉斯最早提出用加1的方法估计没有出现过的现象的概率，所以加法平滑也叫做拉普拉斯平滑。假定训练样本很大时，每个分量x的计数加1造成的估计概率变化可以忽略不计，但可以方便有效的避免零概率问题。因此特征向量中某一个特征的类条件概率密度的计算公式可以改写为：。其中N表示特征xi有几种不同取值    \n代码实现众数填充缺失值dataframe.isnull().any()返回一个Series，某行/列存在缺失值为True，axis=0表示跨行检测，即每一列跨行检测，返回一个与列等宽的Series\ndataframe.fillna(value,inplace=True)表示用value填充DataFrame中NaN的数据，inplace=True表示填充内存中的DataFrame而不是副本。\n# 按照众数填充缺失值def fillMissingColumn(dataframe: pd.DataFrame):    # 检测出有缺失值的列，返回一个Series，有缺失值的列为True，无缺失值的列为False    missing_column = dataframe.isnull().any(axis=0)  # 按列检测    for index, value in missing_column.items():        if value:            print(index + \" needs to fill missing values\")            # 利用众数填充有缺失值的行            dataframe[index].fillna(dataframe[index].mode()[0], inplace=True)  # 一定要设置inplace=True 修改内存的值\n\n处理分类数据将class(A,B,C,D,E)利用func函数映射到1-5。mapper定义了映射字典，将分类特征映射到对应的整数。最后利用dataframe.replace(dict)替换\n# 处理nominal型的分类数据-industry：一共有14类 使用one-hot编码太大，还是采用简单编码# 处理ordinary型的分类数据：employer_type class work_yeardef classifyOrdinary(dataframe: pd.DataFrame):    # 创建class特征 映射器，将A-1,B-2...F-6    func = lambda x: ord(x) - 64  # ord()将字母转变为ASCII码    # 将class特征分类    dataframe['class'] = dataframe['class'].apply(func)    # 创建编码映射器    mapper = {        'industry': {            '金融业': 0,            '电力、热力生产供应业': 1,            '公共服务、社会组织': 2,            '住宿和餐饮业': 3,            '信息传输、软件和信息技术服务业': 4,            '文化和体育业': 5,            '建筑业': 6,            '房地产业': 7,            '采矿业': 8,            '交通运输、仓储和邮政业': 9,            '农、林、牧、渔业': 10,            '制造业': 11,            '批发和零售业': 12,            '国际组织': 13        },        'work_year': {            '&lt; 1 year': 0,            '1 year': 1,            '2 years': 2,            '3 years': 3,            '4 years': 4,            '5 years': 5,            '6 years': 6,            '7 years': 7,            '8 years': 8,            '9 years': 9,            '10+ years': 10,        },        'employer_type': {            '普通企业': 1,            '幼教与中小学校': 2,            '政府机构': 3,            '上市企业': 4,            '高等教育机构': 5,            '世界五百强': 6        }    }    # 离散化    dataframe = dataframe.replace(mapper, inplace=True)\n\n离散化​参考【4】中的博客，利用scorecardpy包先计算出分箱区间：\nimport pandas as pdimport scorecardpy as sc # 导入两列数据df = pd.DataFrame({'年龄': [29,7,49,12,50,34,36,75,61,20,3,11],                   'Y'   : [0,0,1,1,0,1,0,1,1,0,0,0]})bins = sc.woebin(df, y='Y', method='chimerge')  # 卡方分箱sc.woebin_plot(bins)\n\n使用**np.digitize(DataFrame,bin:list)**进行离散化\n\"\"\"    连续的属性值无法计算似然（后验概率）！需要将其离散化——数据分箱    需要离散化的列有：total_loan、interest、monthly_payment、debt_loan_ratio、scoring_low、scoring_high、    recircle_b、recircle_u、early_return_amount_3mon、early_return_amount、title    参考博客：https://blog.csdn.net/Orange_Spotty_Cat/article/details/116485079\"\"\"def discretize(dataframe):    # 使用分箱技术    dataframe['total_loan'] = np.digitize(dataframe['total_loan'], bins=[8000, 21000, 24000, 31000])    # dataframe['interest'] = np.digitize(dataframe['interest'], bins=range(5, 36, 1))  # 4.779-33.979    dataframe['interest'] = np.digitize(dataframe['interest'], bins=[7, 9, 10, 12, 16, 21])    # dataframe['monthly_payment'] = np.digitize(dataframe['monthly_payment'], bins=range(100, 2000, 100))  # 30.44-1503.89    dataframe['monthly_payment'] = np.digitize(dataframe['monthly_payment'], bins=[250, 500])    # dataframe['debt_loan_ratio'] = np.digitize(dataframe['debt_loan_ratio'], bins=range(10, 1000, 10))  # 0-999    dataframe['debt_loan_ratio'] = np.digitize(dataframe['debt_loan_ratio'], bins=[11, 15, 26])    # dataframe['scoring_low'] = np.digitize(dataframe['scoring_low'], bins=range(10, 1000, 10))  # 540-910.9    dataframe['scoring_low'] = np.digitize(dataframe['scoring_low'], bins=[560, 600, 630, 660, 680, 780])    # dataframe['scoring_high'] = np.digitize(dataframe['scoring_high'], bins=range(10, 2000, 10))  # 585.0-1131.818182    dataframe['scoring_high'] = np.digitize(dataframe['scoring_high'], bins=[730, 750, 860])    # dataframe['recircle_b'] = np.digitize(dataframe['recircle_b'], bins=range(10000, 770000, 10000))  # 0.0-779021.0    dataframe['recircle_b'] = np.digitize(dataframe['recircle_b'], bins=[16000])    # dataframe['recircle_u'] = np.digitize(dataframe['recircle_u'], bins=range(1, 120, 1))  # 0.0-120.6153846    dataframe['recircle_u'] = np.digitize(dataframe['recircle_u'], bins=[38, 56, 66, 70])    # dataframe['early_return_amount_3mon'] = np.digitize(dataframe['early_return_amount_3mon'],bins=range(1, 5500, 10))  # 0.0-5523.9    dataframe['early_return_amount_3mon'] = np.digitize(dataframe['early_return_amount_3mon'], bins=[50, 150, 1250])    dataframe['title'] = np.digitize(dataframe['title'], bins=[1])    dataframe['early_return_amount'] = np.digitize(dataframe['early_return_amount'], bins=[5000,10000,15000,20000])\n\n构造分类器为了避免大量重复的计算类条件概率（有很多特征取值相同，计算的类条件概率也是重复的），先用两个numpy的二维数据(利用下标访问)存储所有可能特征值的类条件概率。计算出还剩下29个特征，经过离散化后每个特征的取值类型在100种以内。初始化为0。\n​train.columns.get_loc(columns_label)用于返回某个列名的列索引值。\n\"\"\"    朴素贝叶斯分类，假设每个字段之间相互独立：        小数的连乘可能下溢，因此对p(x|w)*p(w)取对数         为了防止零概率情况使log无意义，使用拉普拉斯平滑技术    定义一个分类器 g(x) = lnp(x|w)+lnp(w)        输入一个特征向量x与一个数据集，输出它的分类\"\"\"# 创建两个dataframe分别缓存isDefault=0与isDefault=1的似然值storeage0 = np.zeros((29,100))storeage1 = np.zeros((29,100))def classifier(x, train: pd.DataFrame):    # 分别统计贷款没违约与贷款违约的情况    type0 = train[train['isDefault'] == 0]    type1 = train[train['isDefault'] == 1]    # 计算行数    sum_type0 = type0.count().values[0]    sum_type1 = type1.count().values[0]    # 计算先验概率    prior_0 = sum_type0 / (sum_type0 + sum_type1)    prior_1 = sum_type1 / (sum_type0 + sum_type1)    # print(str(prior_0) + \" \" + str(prior_1))    # 初始化分类器值(加上lnp(w))    g0 = math.log(prior_0)    g1 = math.log(prior_1)    # print(str(g0) + \" \" + str(g1))    # 计算所有列的似然/类条件概率密度    for column in train.columns:        if column != 'isDefault':  # 去除预测标签的影响            likelihood0, likelihood1 = 0, 0            if storeage0[train.columns.get_loc(column)][int(x[column])] &gt; 0:                # 缓存中已有数据                likelihood0 = storeage0[train.columns.get_loc(column)][int(x[column])]            else:                # 计算拉普拉斯平滑后的似然                likelihood0 = (type0[type0[column] == x[column]].count().values[0] + 1) / (                        sum_type0 + train[column].nunique())                # 按照行-列索引，列—特征值 将数据保存在缓存中                storeage0[train.columns.get_loc(column)][int(x[column])] = likelihood0            # 对 isDefault = 1的训练集数据进行一次同样的操作，计算后验概率            if storeage1[train.columns.get_loc(column)][int(x[column])] &gt; 0:                likelihood1 = storeage1[train.columns.get_loc(column)][int(x[column])]            else:                likelihood1 = (type1[type1[column] == x[column]].count().values[0] + 1) / (                        sum_type1 + train[column].nunique())                storeage1[train.columns.get_loc(column)][int(x[column])] = likelihood1            # 取对数            ln_likelihood0 = math.log(likelihood0)            ln_likelihood1 = math.log(likelihood1)            # print(\"type0: likelihood: \" + str(likelihood0) + \" ln:\" + str(ln_likelihood0))            # print(\"type1: likelihood: \" + str(likelihood1) + \" ln:\" + str(ln_likelihood1))            g0 += ln_likelihood0            g1 += ln_likelihood1            # print('------------------------------------------------------------------')    # print(str(g0) + \" \" + str(g1))    if g0 &gt;= g1:        # 预测为不违约        return 0    else:        return 1\n\n主函数与评估模型​假设贷款不违约(isDefault=0)为正例，贷款违约(isDefault=1)为负例。利用正确率、精度、召回率评估模型。\n# 读取训练集trainSet = pd.read_csv('train.csv')# 读取测试集testSet = pd.read_csv('test.csv')# 删除无关数据列（用户的id，贷款年份(year_of_loan)、贷款发放的月份(issue_date)、借款人最早报告的信用额度开立的月份(earlies_credit_mon)、贷款人申请时邮政编码(post_code)、地区编码(region)等信息，主观判断其对是否违约影响甚微。都是无关属性）trainSet = trainSet.drop(    ['year_of_loan', 'loan_id', 'user_id', 'earlies_credit_mon', 'issue_date', 'post_code', 'region'], axis=1)testSet = testSet.drop(    ['year_of_loan', 'loan_id', 'user_id', 'earlies_credit_mon', 'issue_date', 'post_code', 'region'], axis=1)# 画出频率分布直方图分析数据fig,axs=plt.subplots(8,4,figsize=(40,20),sharex=False,sharey=False)for i in range(10):    for j in range(4):        if i*4+j&lt;31:            axs[i][j].set_title(trainSet.columns[i*4+j])            axs[i][j].bar(x=pd.value_counts(trainSet[trainSet.columns[i*4+j]]).index,height=pd.value_counts(trainSet[trainSet.columns[i*4+j]]).values)plt.show()# 而app_type policy_code 与 f1都是归一化属性，直接去除trainSet = trainSet.drop(['app_type', 'policy_code', 'f1'], axis=1)testSet = testSet.drop(['app_type', 'policy_code', 'f1'], axis=1)# 数据预处理fillMissingColumn(trainSet)classifyOrdinary(trainSet)discretize(trainSet)fillMissingColumn(testSet)classifyOrdinary(testSet)discretize(testSet)# 保存分类器的分类结果isDefault_f = []for index, row in testSet.iterrows():    isDefault_f.append(classifier(row, trainSet))# 将分类结果添加到测试集中testSet['forecast'] = isDefault_f# 创建评估数据集evaluation = testSet[['isDefault', 'forecast']]print(evaluation)# 假设 没有违约（isDefault == 0）为正例TP = 0  # 真正例TN = 0  # 真负例FP = 0  # 假正例FN = 0  # 假负例for index, row in evaluation.iterrows():    if row['isDefault'] == 0 and row['forecast'] == 0:        TP += 1    if row['isDefault'] == 0 and row['forecast'] == 1:        FN += 1    if row['isDefault'] == 1 and row['forecast'] == 0:        FP += 1    if row['isDefault'] == 1 and row['forecast'] == 1:        TN += 1Accuracy = (TP + TN) / (TP + TN + FP + FN)Precision = TP / (TP + FP)Recall = TP / (TP + FN)print(\"正确率: %f\" % Accuracy)print(\"精确率: %f\" % Precision)print(\"召回率: %f\" % Recall)\n\n实验结果​    测试集共有1000行数据，forecast为我们预测的数据。正确率还有待改进。\n\n\n\n参考【1】(3条消息) Lending Club贷款违约预测_Mango的博客-CSDN博客\n【2】机器学习—周志华\n【3】Python机器学习手册：从数据预处理到深度学习\n【4】数据科学猫：数据预处理 之 数据分箱(Binning)_Orange_Spotty_Cat的博客-CSDN博客\n【5】从论文分析，告诉你什么叫 “卡方分箱”？ - 云+社区 - 腾讯云 (tencent.com)\n","categories":["机器学习"],"tags":["机器学习","python"]},{"title":"实验：bagging集成学习预测Titanic数据集","url":"/2022/01/10/%E5%AE%9E%E9%AA%8C%EF%BC%9Abagging%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%E9%A2%84%E6%B5%8BTitanic%E6%95%B0%E6%8D%AE%E9%9B%86/","content":"实验要求用集成方法对数据集进行分类\n\n利用若干算法，针对同一样本数据训练模型，使用投票机制，少数服从多数，用多数算法给出的结果当作最终的决策依据，对Titanic数据集 进行分类，给出在测试集上的精确度； \n\n除了投票法，其他的集成学习方法也可以。 \n\n实验来自kaggle入门赛 https://www.kaggle.com/c/titanic ,可以参考原网站 代码与预处理部分，但与公开代码不同的在于，集成学习所用的基学习 器需要自己实现而不能调用sklearn库。 \n\n数据集的分析是一个开放性问题，可以参考网站中的预处理方式。 \n\n所选算法包括但不限于课堂上学习的模型例如： 决策树 SVM KNN  神经网络\n\n需要在网站上提交，不要求结果很高，但要求模型自己实现，如果有优化可以加分\n\n\n实验思路主要采用bagging集成学习方法，Bagging原理可见另一篇博客(16条消息) 集成学习(ensemble learning)_Sunburst7的博客-CSDN博客。子学习器采用贝叶斯决策论进行决策，对于训练集，我们进行5次有放回的随机抽样，得到5个训练子集，然后用这五个训练子集分别进行决策，得到5个分类结果，再进行投票决定最终的分类结果。\nTitanic数据字典：\n\n\n\nVariable\nDefinition\nKey\n\n\n\nsurvival\nSurvival\n0 = No, 1 = Yes\n\n\npclass\nTicket class，社会地位\nA proxy for socio-economic status (SES)，1 = 1st, 2 = 2nd, 3 = 3rd\n\n\nName\n乘客姓名\n\n\n\nsex\nSex\n\n\n\nAge\nAge in years\n\n\n\nsibsp\n# of siblings / spouses aboard the Titanic\n\n\n\nparch\n# of parents / children aboard the Titanic\n\n\n\nticket\nTicket number\n\n\n\nfare\nPassenger fare\n\n\n\ncabin\nCabin number\n\n\n\nembarked\nPort of Embarkation 出发港\nC = Cherbourg, Q = Queenstown, S = Southampton\n\n\n连接训练集与测试集，查看缺失值：\ntrain = pd.read_csv(\"train.csv\")test = pd.read_csv(\"test.csv\")df = pd.concat([train, test], axis=0)df = df.set_index('PassengerId')print(df.info())\n\n控制台输出：\nInt64Index: 1309 entries, 1 to 1309Data columns (total 11 columns): #   Column    Non-Null Count  Dtype  ---  ------    --------------  -----   0   Survived  891 non-null    float64 1   Pclass    1309 non-null   int64   2   Name      1309 non-null   object    3   Sex       1309 non-null   object  4   Age       1046 non-null   float64 5   SibSp     1309 non-null   int64   6   Parch     1309 non-null   int64   7   Ticket    1309 non-null   object    8   Fare      1308 non-null   float64 9   Cabin     295 non-null    object   10  Embarked  1307 non-null   object dtypes: float64(3), int64(3), object(5)\n\nCabin特征因为有太多的缺失值，所以决定舍弃它，本次不进行语义分析，因此乘客姓名Name，船票编号Ticket人人各不相同，也被舍弃。\n\n总体分析Age,Pclass,Sex对于Survived的影响：\n  import seaborn as snssns.set_theme(font_scale=1.5)sns.displot(data=df,hue='Sex', x='Age',row='Survived',col='Pclass')plt.show()\n\n  \n\n  第一行的乘客是那些没有存活下来的乘客，似乎大部分是来自Pclass=3的男性。幸存者大多是女性(第二行)。因此，Age和Pclass是生存的重要指标，年龄则不那么重要。\n\n仔细分析Age-Survived\n  sns.catplot(data=df,hue='Sex', x='Kind',row='Survived',col='Pclass', kind='count')plt.show()\n\n  `Pclass=1或2` 所有孩子都活下来了，也许这对我们有所帮助，所以我们保留`Age`这个特征，似乎儿童作为一个整体(年龄&lt;=15岁以上)比成年人生存得更好。因此可以粗略的将年龄分为成年人（`Age&gt;15`）与未成年人（`Age`&lt;=15)\n观察出发港Embarked\n  sns.catplot(data=df,hue='Sex', x='Embarked',col='Survived',kind='count')plt.show()\n\n  \n\n  从Southampton 出发的男性存活得较少(也许他们是在三等舱?)，但大约75%的南安普顿女性存活了下来，对于从Cherbourg出发的人来说，关系就相反了。所以Embarked也很重要。\n\nFare表示船票价格：乘客要花更多的钱才能坐头等舱，但这对他们的生存并没有太大的帮助。去除该特征\n\n\n因此留下的有意义的特征属性如下：\n\n\n\nVariable\nDefinition\nKey\n\n\n\nsurvival\nSurvival，预测标签\n0 = No, 1 = Yes\n\n\npclass\nTicket class，社会地位\nA proxy for socio-economic status (SES)，1 = 1st, 2 = 2nd, 3 = 3rd\n\n\nsex\nSex\n\n\n\nAge\nAge in years\n\n\n\nsibsp\n# of siblings / spouses aboard the Titanic\n\n\n\nparch\n# of parents / children aboard the Titanic\n\n\n\nembarked\nPort of Embarkation 出发港\nC = Cherbourg, Q = Queenstown, S = Southampton\n\n\n实验代码import mathimport numpy as npimport pandas as pdimport seaborn as snsimport matplotlib.pyplot as plt# 按照众数填充缺失值def fillMissingColumn(dataframe: pd.DataFrame):    # 检测出有缺失值的列，返回一个Series，有缺失值的列为True，无缺失值的列为False    missing_column = dataframe.isnull().any(axis=0)  # 按列检测    for index, value in missing_column.items():        if value:            print(index + \" needs to fill missing values\")            # 利用众数填充有缺失值的行            dataframe[index].fillna(dataframe[index].mode()[0], inplace=True)  # 一定要设置inplace=True 修改内存的值# 贝叶斯分类器，输入一个向量，输出预测标签           def classifier(x, train: pd.DataFrame):    # 分别存活的情况与没有存活的情况 0-死亡 1-存活    type0 = train[train['Survived'] == 0]    type1 = train[train['Survived'] == 1]    # 计算行数    sum_type0 = type0.count().values[0]    sum_type1 = type1.count().values[0]    # 计算先验概率    prior_0 = sum_type0/(sum_type0+sum_type1)    prior_1 = sum_type1/(sum_type0+sum_type1)    # print(str(prior_0) + \" \" + str(prior_1))    # 初始化分类器值(加上lnp(w))    g0 = math.log(prior_0)    g1 = math.log(prior_1)    # print(str(g0) + \" \" + str(g1))    # 计算所有列的似然/类条件概率密度    for column in train.columns:        if column != 'Survived':# 去除预测标签的影响            # print(column)            # 计算拉普拉斯平滑后的似然            likelihood0 = (type0[type0[column] == x[column]].count().values[0] + 1) / (sum_type0 + train[column].nunique())            likelihood1 = (type1[type1[column] == x[column]].count().values[0] + 1) / (sum_type1 + train[column].nunique())            # 取对数            ln_likelihood0 = math.log(likelihood0)            ln_likelihood1 = math.log(likelihood1)            # print(\"type0: likelihood: \" + str(likelihood0) + \" ln:\" + str(ln_likelihood0))            # print(\"type1: likelihood: \" + str(likelihood1) + \" ln:\" + str(ln_likelihood1))            g0 += ln_likelihood0            g1 += ln_likelihood1            # print('------------------------------------------------------------------')    # print(str(g0) + \" \" + str(g1))    if g0&gt;=g1:        # 预测为无法存活        return 0    else:        return 1# Load_Datatrain = pd.read_csv(\"train.csv\")test = pd.read_csv(\"test.csv\")# EDA 数据预处理df = pd.concat([train, test], axis=0)df = df.set_index('PassengerId')df['Age'] = df['Age'].map(lambda x:'Adult' if x &gt;15 else 'Child')df['Embarked'] = df['Embarked'].fillna('S')df = df.drop(['Name', 'Ticket', 'Cabin','Fare'], axis=1)# 填充缺失值fillMissingColumn(df)print(df.info())mapper = {    'Sex':{        'male':0,        'female':1    },    'Embarked':{        'C':0,        'Q':1,        'S':2,    }}# 特征数字化df.replace(mapper,inplace=True)print(df.head())# 分割训练集与测试集test = df.iloc[891:1309:1,:]train = df.iloc[0:891:1,:]SamplesArray = []for i in range(5):    # 有放回的随机抽样 抽样占比63%    SamplesArray.append(train.sample(frac=1,replace=True,axis=0))# 保存分类器的分类结果isSurvived = []for index,row in test.iterrows():    tmp = []    # 对于每一个测试集数据，放到5个子分类器中进行分类，保存子分类器分类结果    for i in range(5):        tmp.append(classifier(row,SamplesArray[i]))    isSurvived.append(tmp)# 投票决定最终预测vote = []for item in isSurvived:    SurvivedCount = str(item).count('1')    if SurvivedCount &gt; 2:        vote.append(1)    else:        vote.append(0)test['forecast_by_vote'] = vote# 输出预测结果for index,row in test.iterrows():    if row['forecast_by_vote'] == 1:        print(\"PassengerId: \" + str(index) + \", forecast: can Survive\")    else:        print(\"PassengerId: \" + str(index) + \", forecast: can't Survive\")# 写入CSVprint('write prediction to file:submission.csv')test['PassengerId'] = test.indextest[['PassengerId','forecast_by_vote']].to_csv('submission.csv', index=False)\n\n实验结果运行控制台截图，控制台输出每个测试乘客的预测结果，同时将结果写入到submission.csv中：\n\n\n在Kaggle网站上提交，正确率为76%，还有待提高：\n","categories":["机器学习"],"tags":["机器学习","python"]},{"title":"实验：决策树完成鸢尾花卉品种预测","url":"/2022/01/10/%E5%AE%9E%E9%AA%8C%EF%BC%9A%E5%86%B3%E7%AD%96%E6%A0%91%E5%AE%8C%E6%88%90%E9%B8%A2%E5%B0%BE%E8%8A%B1%E5%8D%89%E5%93%81%E7%A7%8D%E9%A2%84%E6%B5%8B/","content":"[TOC]\n实验要求本实验通过鸢尾花数据集iris.csv来实现对决策树进一步的了解。其中， Iris鸢尾花数据集是一个经典数据集，在统计学习和机器学习领域都经常被用作示例。数据集内包含3类共150条记录，每类各50个数据，每条记录都有4项特征：花萼长度、花萼宽度、花瓣长度、花瓣宽度，可以通过这4个特征预测鸢尾花卉属于iris-setosa, iris-versicolour, iris-virginica三个类别中的 哪一品种。Iris数据集样例如下图所示：\n\n\n本实验将五分之四的数据集作为训练集对决策树模型进行训练；将剩余五 分之一的数据集作为测试集，采用训练好的决策树模型对其进行预测。训练集 与测试集的数据随机选取。本实验采用准确率(accuracy)作为模型的评估函数：预测结果正确的数量占样本总数，(TP+TN)/(TP+TN+FP+FN)。\n【实验要求】  \n\n本实验要求输出测试集各样本的预测标签和真实标签，并计算模型准确率。(选做)另外，给出 3 个可视化预测结果。 \n\n决策树算法可以分别尝试 ID3,C4.5,cart树，并评判效果。 \n\n（选做）：对你的决策树模型进行预剪枝与后剪枝 \n\n（选做）：分别做 c4.5 和 cart 树的剪枝并比较不同。\n\n\n实验思路分析数据结构，因为没有每个样本独有的属性（例如学生ID），决定采用ID3决策树（ID3决策树的信息增益偏向于可能值较多的属性）。采用信息增益Information Gain确定划分的最优特征，对于保存树的结构方面，采用字典的形式保存，以下方字典形式为例：\n{    \"PetalWidth\": {        \"0\": \"Iris-setosa\",        \"1\": {            \"PetalLength\": {                \"0\": \"Iris-setosa\",                \"1\": \"Iris-versicolor\",                \"2\": {                    \"SepalLength\": {                        \"1\": \"Iris-virginica\",                        \"2\": \"Iris-versicolor\"                    }                }            }        },        \"2\": {            \"SepalLength\": {                \"0\": \"Iris-virginica\",                \"1\": \"Iris-virginica\",                \"2\": {                    \"SepalWidth\": {                        \"0\": \"Iris-virginica\",                        \"1\": \"Iris-virginica\"                    }                }            }        }    }}\n\n由于数据是连续的，在训练决策树之前需要将其离散化，利用python中seaborn库观察数据分布：\nimport pandas as pdimport matplotlib.pyplot as pltimport seaborn as sbData = pd.read_csv('iris.csv')sb.pairplot(Data.dropna(), hue='Species')plt.show()\n\n\n\n因此我们确定数据的离散边界：\n\n\n\n离散特征\\离散后的值\n0\n1\n2\n\n\n\nSepalLength\n0-5.5\n5.5-6.3\n6.3-inf\n\n\nSepalWidth\n0.3.2\n3.2-inf\n\n\n\nPetalLength\n0-2\n2-4.9\n4.9-inf\n\n\nPetalWidth\n0-0.6\n0.6-1.7\n1.7-inf\n\n\n为了保证结果的随机性，训练集与测试集的划分采用随机采样，每种花随机抽取10个共30个测试集样本，其余为训练集样本：\n# 分割训练集与测试集def split_train_test(data:pd.DataFrame):    # 随机采样    test_index = random.sample(range(50),10)    test_index.extend(random.sample(range(50,100),10))    test_index.extend(random.sample(range(100,150),10))    print(test_index)    testSet = data.iloc[test_index]    train_index = list(range(150))    for index in test_index:        train_index.remove(index)    # 划分训练集    trainSet = data.iloc[train_index]    return trainSet,testSet\n\n实验代码\nsplit_train_test：分割训练集与测试集\nShannonEntropy：计算一个Dataframe关于forecast_label列的信息熵\nInformationGain：计算一个Dataframe中label标签关于forecast_label的信息增益\ncreateTree：递归生成决策树的过程\ndecision：对一个样本进行决策\n\nimport pandas as pdimport numpy as npimport matplotlib.pyplot as pltimport seaborn as sbimport jsonimport randomdef split_train_test(data:pd.DataFrame):    \"\"\"    分割训练集与测试集    :param data: 总的数据集    :return: 返回训练集与测试集    \"\"\"    # 随机采样    test_index = random.sample(range(50),10)    test_index.extend(random.sample(range(50,100),10))    test_index.extend(random.sample(range(100,150),10))    print(test_index)    testSet = data.iloc[test_index]    train_index = list(range(150))    for index in test_index:        train_index.remove(index)    # 划分训练集    trainSet = data.iloc[train_index]    return trainSet,testSetdef ShannonEntropy(data:pd.DataFrame,forecast_label:str)-&gt;float:    \"\"\"    计算一个数据集关于某个标签(这里是预测标签)的信息熵    :param data: 数据集    :param forecast_label: 预测标签    :return: 返回这个数据集的信息熵    \"\"\"    total = data.shape[0]    kinds = data[forecast_label].value_counts()    Entropy = 0.0    # 对于每种预测标签 计算pk*log(pk)    for i in range(kinds.shape[0]):        # 计算每种预测标签的比例        prior_probability = kinds[i]/total        # 计算信息熵        Entropy += (prior_probability * np.log2(prior_probability))    return -Entropydef InformationGain(data:pd.DataFrame,label:str,forecast_label:str)-&gt;float:    \"\"\"    计算label标签关于forecast_label的信息增益    :param data: 数据集    :param label: 计算标签    :param forecast_label: 预测标签    :return: 信息增益    \"\"\"    # 计算总的信息熵Entropy(S)    total_entropy = ShannonEntropy(data,forecast_label)    # 初始化信息增益    gain = total_entropy    # 按照计算标签分组    sub_frame = data[[label,'Species']]    group = sub_frame.groupby(label)    # 计算信息增益    for key, df in group:        gain -= (df.shape[0]/data.shape[0]) * ShannonEntropy(df,'Species')    return gaindef createTree(data:pd.DataFrame)-&gt;dict:    \"\"\"    递归的创建决策树    :param data: 训练集数据    :return: 返回一个字典表示决策树    \"\"\"    # 该分支下的实例只有一种分类    if len(data['Species'].value_counts()) == 1:        return data['Species'].iloc[0]    # 初始化最优信息增益与最优属性    bestGain = 0    bestFeature = -1    # 对于每种属性计算信息增益，选出信息增益最大的一列    for column in data:        if column != 'Species':            gain = InformationGain(data, column, 'Species')            if bestGain &lt; gain:                bestGain = gain                bestFeature = column    # 数据集中所有数据都相同，但种类不同，返回最多数量的种类    if bestFeature == -1:        valueCount = data['Species'].value_counts()        return valueCount.index[0]    # 初始化一个字典    myTree = {bestFeature: {}}    # 统计出最佳属性的所有可能取值    valueList = set(data[bestFeature])    for value in valueList:        # 递归的构造子树        myTree[bestFeature][value] = createTree(data[data[bestFeature] == value])    return myTreedef decision(tree:dict,testVector:pd.Series):    \"\"\"    预测一个测试集样本的类别    :param tree: 生成的决策树    :param testVector: 测试数据向量    :return: 返回预测标签    \"\"\"    # 初始化预测标签    forecastLabel = 0    # 获取当前决策树第一个节点属性    firstFeature = next(iter(tree))    # 获取子树    childTree = tree[firstFeature]    # 对子树中不同的可能值检测是否相等    for key in childTree.keys():        # 满足条件深入到下一层        if testVector[firstFeature] == key:            # 下一层是分支节点            if type(childTree[key]) == dict :                forecastLabel = decision(childTree[key],testVector)            # 下一层是叶节点            else:                forecastLabel = childTree[key]    return forecastLabelif __name__ == '__main__' :    Data = pd.read_csv('iris.csv')    # 画出统计分布图，统计每种类别的特征    # sb.pairplot(Data.dropna(), hue='Species')    # plt.show()    # 数据离散化处理    Data['SepalLength'] = np.digitize(Data['SepalLength'],bins=[5.5,6.3])    Data['SepalWidth'] = np.digitize(Data['SepalWidth'],bins=[3.2])    Data['PetalLength'] = np.digitize(Data['PetalLength'],bins=[2,4.9])    Data['PetalWidth'] = np.digitize(Data['PetalWidth'],bins=[0.6,1.7])    # 数据离散化的字典    discrete_dict = {        'SepalLength' : {'0-5.5':0,'5.5-6.3':1,'6.3-inf':2},        'SepalWidth' : {'0.3.2':0,'3.2-inf':1},        'PetalLength' : {'0-2':0,'2-4.9':1,'4.9-inf':2},        'PetalWidth' : {'0-0.6':0,'0.6-1.7':1,'1.7-inf':2}    }    print('数据离散化字典：')    print(json.dumps(discrete_dict, indent=4))    # 分出训练集与测试集    train_set,test_set = split_train_test(Data)    # 训练出决策树    tree = createTree(train_set)    print(\"决策树字典表示：\")    print(json.dumps(tree, indent=4, sort_keys=True))    # 初始化统计参数    T = 0    N = 0    for i in range(test_set.shape[0]):        print('================================================')        vector = test_set.iloc[i, :-1]        sl = vector['SepalLength']        sw = vector['SepalWidth']        pl = vector['PetalLength']        pw = vector['PetalWidth']        trueLabel = test_set.iloc[i]['Species']        print(f'离散化后的测试数据:SepalLength={sl},SepalWidth={sw},PetalLength={pl},PetalWidth={pw},真实标签={trueLabel}')        forecastLabel= decision(tree,vector)        if forecastLabel == trueLabel:            T+=1            print(f'预测为{forecastLabel},预测正确')        else:            N+=1            print(f'预测为{forecastLabel},预测错误')    print('-------------------------------------------------------------')    print(f'决策树预测准确率为:'+str(T/(T+N)))\n\n实验结果与分析运行程序后的控制台输出：\n数据离散化字典：{    \"SepalLength\": {        \"0-5.5\": 0,        \"5.5-6.3\": 1,        \"6.3-inf\": 2    },    \"SepalWidth\": {        \"0.3.2\": 0,        \"3.2-inf\": 1    },    \"PetalLength\": {        \"0-2\": 0,        \"2-4.9\": 1,        \"4.9-inf\": 2    },    \"PetalWidth\": {        \"0-0.6\": 0,        \"0.6-1.7\": 1,        \"1.7-inf\": 2    }}随机训练集index: [38, 28, 26, 44, 40, 19, 2, 18, 7, 46, 68, 79, 50, 97, 65, 88, 69, 81, 92, 95, 126, 132, 137, 131, 110, 124, 133, 116, 125, 143]决策树字典表示：{    \"PetalLength\": {        \"0\": \"Iris-setosa\",        \"1\": {            \"PetalWidth\": {                \"1\": \"Iris-versicolor\",                \"2\": {                    \"SepalWidth\": {                        \"0\": \"Iris-virginica\",                        \"1\": \"Iris-versicolor\"                    }                }            }        },        \"2\": {            \"PetalWidth\": {                \"1\": {                    \"SepalLength\": {                        \"1\": \"Iris-virginica\",                        \"2\": \"Iris-versicolor\"                    }                },                \"2\": {                    \"SepalLength\": {                        \"1\": \"Iris-virginica\",                        \"2\": {                            \"SepalWidth\": {                                \"0\": \"Iris-virginica\",                                \"1\": \"Iris-virginica\"                            }                        }                    }                }            }        }    }}================================================离散化后的测试数据:SepalLength=0,SepalWidth=0,PetalLength=0,PetalWidth=0,真实标签=Iris-setosa预测为Iris-setosa,预测正确================================================离散化后的测试数据:SepalLength=0,SepalWidth=1,PetalLength=0,PetalWidth=0,真实标签=Iris-setosa预测为Iris-setosa,预测正确================================================离散化后的测试数据:SepalLength=0,SepalWidth=1,PetalLength=0,PetalWidth=0,真实标签=Iris-setosa预测为Iris-setosa,预测正确================================================离散化后的测试数据:SepalLength=0,SepalWidth=1,PetalLength=0,PetalWidth=0,真实标签=Iris-setosa预测为Iris-setosa,预测正确================================================离散化后的测试数据:SepalLength=0,SepalWidth=1,PetalLength=0,PetalWidth=0,真实标签=Iris-setosa预测为Iris-setosa,预测正确================================================离散化后的测试数据:SepalLength=0,SepalWidth=1,PetalLength=0,PetalWidth=0,真实标签=Iris-setosa预测为Iris-setosa,预测正确================================================离散化后的测试数据:SepalLength=0,SepalWidth=1,PetalLength=0,PetalWidth=0,真实标签=Iris-setosa预测为Iris-setosa,预测正确================================================离散化后的测试数据:SepalLength=1,SepalWidth=1,PetalLength=0,PetalWidth=0,真实标签=Iris-setosa预测为Iris-setosa,预测正确================================================离散化后的测试数据:SepalLength=0,SepalWidth=1,PetalLength=0,PetalWidth=0,真实标签=Iris-setosa预测为Iris-setosa,预测正确================================================离散化后的测试数据:SepalLength=0,SepalWidth=1,PetalLength=0,PetalWidth=0,真实标签=Iris-setosa预测为Iris-setosa,预测正确================================================离散化后的测试数据:SepalLength=1,SepalWidth=0,PetalLength=1,PetalWidth=1,真实标签=Iris-versicolor预测为Iris-versicolor,预测正确================================================离散化后的测试数据:SepalLength=1,SepalWidth=0,PetalLength=1,PetalWidth=1,真实标签=Iris-versicolor预测为Iris-versicolor,预测正确================================================离散化后的测试数据:SepalLength=2,SepalWidth=1,PetalLength=1,PetalWidth=1,真实标签=Iris-versicolor预测为Iris-versicolor,预测正确================================================离散化后的测试数据:SepalLength=1,SepalWidth=0,PetalLength=1,PetalWidth=1,真实标签=Iris-versicolor预测为Iris-versicolor,预测正确================================================离散化后的测试数据:SepalLength=2,SepalWidth=0,PetalLength=1,PetalWidth=1,真实标签=Iris-versicolor预测为Iris-versicolor,预测正确================================================离散化后的测试数据:SepalLength=1,SepalWidth=0,PetalLength=1,PetalWidth=1,真实标签=Iris-versicolor预测为Iris-versicolor,预测正确================================================离散化后的测试数据:SepalLength=1,SepalWidth=0,PetalLength=1,PetalWidth=1,真实标签=Iris-versicolor预测为Iris-versicolor,预测正确================================================离散化后的测试数据:SepalLength=1,SepalWidth=0,PetalLength=1,PetalWidth=1,真实标签=Iris-versicolor预测为Iris-versicolor,预测正确================================================离散化后的测试数据:SepalLength=1,SepalWidth=0,PetalLength=1,PetalWidth=1,真实标签=Iris-versicolor预测为Iris-versicolor,预测正确================================================离散化后的测试数据:SepalLength=1,SepalWidth=0,PetalLength=1,PetalWidth=1,真实标签=Iris-versicolor预测为Iris-versicolor,预测正确================================================离散化后的测试数据:SepalLength=1,SepalWidth=0,PetalLength=1,PetalWidth=2,真实标签=Iris-virginica预测为Iris-virginica,预测正确================================================离散化后的测试数据:SepalLength=2,SepalWidth=0,PetalLength=2,PetalWidth=2,真实标签=Iris-virginica预测为Iris-virginica,预测正确================================================离散化后的测试数据:SepalLength=2,SepalWidth=0,PetalLength=2,PetalWidth=2,真实标签=Iris-virginica预测为Iris-virginica,预测正确================================================离散化后的测试数据:SepalLength=2,SepalWidth=1,PetalLength=2,PetalWidth=2,真实标签=Iris-virginica预测为Iris-virginica,预测正确================================================离散化后的测试数据:SepalLength=2,SepalWidth=1,PetalLength=2,PetalWidth=2,真实标签=Iris-virginica预测为Iris-virginica,预测正确================================================离散化后的测试数据:SepalLength=2,SepalWidth=1,PetalLength=2,PetalWidth=2,真实标签=Iris-virginica预测为Iris-virginica,预测正确================================================离散化后的测试数据:SepalLength=2,SepalWidth=0,PetalLength=2,PetalWidth=1,真实标签=Iris-virginica预测为Iris-versicolor,预测错误================================================离散化后的测试数据:SepalLength=2,SepalWidth=0,PetalLength=2,PetalWidth=2,真实标签=Iris-virginica预测为Iris-virginica,预测正确================================================离散化后的测试数据:SepalLength=2,SepalWidth=1,PetalLength=2,PetalWidth=2,真实标签=Iris-virginica预测为Iris-virginica,预测正确================================================离散化后的测试数据:SepalLength=2,SepalWidth=1,PetalLength=2,PetalWidth=2,真实标签=Iris-virginica预测为Iris-virginica,预测正确-------------------------------------------------------------决策树预测准确率为:0.9666666666666667\n\n观察决策树的结构，SepalWidth=0或SepalWidth=1时的决策结果相同，可以通过剪枝的操作减去多余的子树。同时对数据的分布进行分析，SepalWidth与SepalLength的数据重合部分大，对这两个数据进行预剪枝的效果可能更好。\n参考【1】机器学习实战\n【2】机器学习项目实战–基于鸢尾花数据集（python代码，多种算法对比：决策树、SVM、k近邻）_西南交大-Liu_z的博客-CSDN博客\n","categories":["机器学习"],"tags":["机器学习","python"]},{"title":"实验：神经网络预测Fashion—MNIST数据集","url":"/2022/01/10/%E5%AE%9E%E9%AA%8C%EF%BC%9A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E9%A2%84%E6%B5%8BFashion%E2%80%94MNIST%E6%95%B0%E6%8D%AE%E9%9B%86/","content":"实验数据Fashion-MNIST数据集，数据集中包含 60000 张训练样本，10000 张测试 样本，可将训练样本划分为49000 张样本的训练集和1000 张样本的验证集，测 试集可只取1000 张测试样本。其中每个样本都是 28×28 像素的灰度照片，每 个像素点包括RGB三个数值，数值范围0 ~ 255，所有照片分属10个不同的类别。\n\n\n灰度与像素值的关系：\n\n图像的灰度化： 灰度就是没有色彩，RGB色彩分量全部相等。图像的灰度化就是让像素点矩阵中的每一个像素点都满足关系：R=G=B，此时的这个值叫做灰度值。如RGB(100,100,100)就代表灰度值为100,RGB(50,50,50)代表灰度值为50。\n\n灰度值与像素值的关系： 如果对于一张本身就是灰度图像（8位灰度图像）来说，他的像素值就是它的灰度值，如果是一张彩色图像，则它的灰度值需要经过函数映射来得到。灰度图像是由纯黑和纯白来过渡得到的，在黑色中加入白色就得到灰色，纯黑和纯白按不同的比例来混合就得到不同的灰度值。R=G=B=255为白色，R=G=B=0为黑色，R=G=B=小于255的某个整数时，此时就为某个灰度值。\n\n\n实验要求\n用神经网络对给定的数据集进行分类，画出loss图，给出在测试集上的精确度；\n\n不能使用 pytorch 等框架，也不能使用库函数，所有算法都要自己实现\n\n神经网络结构图如下图所示：\n \n整个神经网络包括 3 层——输入层，隐藏层，输出层。输入层有28x28x3个神经元，隐藏层有 50个神经元，输出层有 10 个神经元（对应 10 个类别）。\n\n附加：可以试着修改隐藏层神经元数，层数，学习率，正则化权重等参数探究参数对实验结果的影响\n\n\n实验思路与代码实验要求设计一个三层的全连接神经网络，实现分类的功能。实验原理请见我的博客：神经网络基础与反向传播_Sunburst7的博客-CSDN博客\n神经网络的每次迭代主要包括以下几个步骤：\n\n传入训练图片灰度矩阵/数组\n进行前馈运算，计算出输出层10个神经元预测的标签\n与期望的标签计算Loss，更新隐含层（Hidden）到输出层（Output）的权值\n计算隐含层的敏感度，更新输入层（Input）到隐含层的权值\n传入测试图片灰度矩阵，进行预测。\n统计\n\n起初我的设计充满了面向对象的思想，导致在进行大数据集的运算时消耗时间很长，无法训练神经网络，受到参考【2】【3】【4】中博客的启发，改变思路，采用矩阵运算的思想成功训练出一个良好的神经网络。\n初始的设想我准备编写一个神经元类表示单个神经元，该神经元有以下属性，手绘原理图：\n\n\n\n权值数组Weights：上一层与该神经元相连的所有神经元的权值\n\n偏置Bias\n\n输入值数组Inputs与输出值Output\n\n激活函数\n\n敏感度：定义可见实验原理博客\n\n神经元类型Type：标识是输入层还是输出层还是隐含层神经元\n\n\n每个神经元有两个函数，分别代表前馈与反向传播。\n\n前馈：将输入值与权值进行向量的内积，加上偏置，在传入激活函数，输出最后的值\n反向传播：对于输入层与输出层不同，利用传入的敏感度数组与权值数组更新自身的权值矩阵。\n\nimport numpy as npdef sigmoid(x):    if x&gt;=0:      #对sigmoid函数的优化，避免了出现极大的数据溢出        return 1.0/(1+np.exp(-x))    else:        return np.exp(x)/(1+np.exp(x))class Neuron:    def __init__(self, weights, bias, ntype):        # 上一层与之相连的weights array        self.weights = weights        # 偏置 bias        self.bias = bias        # input array        self.inputs = 0        # 总的输出 z_k        self.output = 0        # 一个神经元的敏感度        self.sensitivity = 0.0        # 神经元的类型:Input Hidden Output        self.ntype = ntype    def __str__(self):        return \"weights: \\n\"+str(self.weights)+\"\\nbias:\\n\"+str(self.bias)+\"\\ninputs:\\n\"+str(self.inputs)+\"\\noutput:\\n\"+str(self.output)+\"\\nsensitivity\\n\"+str(self.sensitivity)+\"\\ntype:\\n\"+str(self.ntype)    def feedForward(self, inputs:np.array):        \"\"\"        前馈        :param inputs: 输入的向量        :return:        \"\"\"        self.inputs = inputs        # weight inputs, add bias, then use the activation function        total = np.dot(self.weights, self.inputs) + self.bias        # 计算神经元的输出        # 如果是输入层，不需要带入激活函数f        if self.ntype == 'Input':            self.output = total        else:            self.output = sigmoid(total)        return self.output    def backPropagation(self ,eta ,tk ,sensitivities ,weights):        \"\"\"        反向传递更新权值        :param eta: 学习率        :param tk: 真实标签        :param sensitivities: 该神经元如果不是输出层，隐含层下一层所有神经元的敏感度        :param weights: 该神经元如果不是输出层，与该神经元相连的下一层的所有权值        :return:        \"\"\"        if self.ntype == 'Output':            # 计算f'(net)=f(net)*(1-f(net))            derivative_f = self.output*(1-self.output)            # 计算loss'(zk)=-(tk-zk)            derivative_loss = -(tk - self.output)            # 计算该神经元的敏感度 sensitivity = -f'(net)*loss'(zk)            self.sensitivity = -derivative_f*derivative_loss            # 更新权重            self.weights = self.weights + eta*self.sensitivity*np.array(self.inputs)        elif self.ntype == 'Hidden':            # 计算f'(net)=f(net)*(1-f(net))            derivative_f = self.output * (1 - self.output)            # 计算隐含层单元的敏感度 sensitivity = f'(net)*&lt;下一层所有神经元的敏感度，该神经元与下一层相连的权重&gt;            self.sensitivity = derivative_f*np.dot(sensitivities,weights)            # 更新权重            self.weights = self.weights + eta*self.sensitivity*np.array(self.inputs)        else:            return\n\n再编写一个神经网络类构造三层神经元并进行feedForward与backPropagation的参数传递工作：\nimport numpy as npimport osimport gzipfrom exp4.Neuron1 import Neurondef sigmoid(x):    if x&gt;=0:      #对sigmoid函数的优化，避免了出现极大的数据溢出        return 1.0/(1+np.exp(-x))    else:        return np.exp(x)/(1+np.exp(x))# 定义加载数据的函数，加载4个zip格式文件def load_data(data_folder):  files = [      'train-labels-idx1-ubyte.gz', 'train-images-idx3-ubyte.gz',      't10k-labels-idx1-ubyte.gz', 't10k-images-idx3-ubyte.gz'  ]  paths = []  for fname in files:    paths.append(os.path.join(data_folder,fname))  with gzip.open(paths[0], 'rb') as lbpath:    y_train = np.frombuffer(lbpath.read(), np.uint8, offset=8)  with gzip.open(paths[1], 'rb') as imgpath:    x_train = np.frombuffer(        imgpath.read(), np.uint8, offset=16).reshape(len(y_train), 28, 28)  with gzip.open(paths[2], 'rb') as lbpath:    y_test = np.frombuffer(lbpath.read(), np.uint8, offset=8)  with gzip.open(paths[3], 'rb') as imgpath:    x_test = np.frombuffer(        imgpath.read(), np.uint8, offset=16).reshape(len(y_test), 28, 28)  return (x_train, y_train), (x_test, y_test)def makeTrueLabels(trueLabel):    \"\"\"    生成传递给输出层的期望标签数组    :param trueLabel:    :return:    \"\"\"    labels = []    for i in range(10):        if trueLabel == i:            labels.append(1)        else:            labels.append(0)    return labelsclass NeuronNetwork:    def __init__(self,train_images,train_labels,test_images,test_labels):        self.inputLayer = []        self.hiddenLayer = []        self.outputLayer = []        self.train_images = train_images        self.train_labels = train_labels        self.test_images = test_images        self.test_labels = test_labels        # 从输入层到隐含层权值的初始化        initWeight_1 = np.random.uniform(-1/np.sqrt(784),1/np.sqrt(784),(50,784))        # 从隐含层到输出层权值的初始化        initWeight_2 = np.random.uniform(-1/np.sqrt(50),1/np.sqrt(50),(10,50))        # 偏置初始化        bias = np.random.normal(0,0.5,2)        # 初始化输入层        for i in range(784):            neuron = Neuron(1,0,ntype='Input')            self.inputLayer.append(neuron)        # 初始化隐藏层        for i in range(50):            neuron = Neuron(initWeight_1[i],bias[0],ntype='Hidden')            self.hiddenLayer.append(neuron)        # 初始化输出层        for i in range(10):            neuron = Neuron(initWeight_2[i],bias[1],ntype='Output')            self.outputLayer.append(neuron)    def iteration(self):        T = 0        N = 0        # 依次将60000章图片训练一边        for m in range(self.train_images.shape[0]):            # 输入层(784个神经元)的输出            y_i = []            # 对于每张图片的784个像素点            for row in range(self.train_images.shape[1]):                for col in range(self.train_images.shape[2]):                    y_i.append(                        self.inputLayer[28 * row + col].feedForward(inputs=np.array(train_images[m][row][col]))                    )            # 隐藏层(50个神经元)的输出            y_h = []            for hidden_neuron in self.hiddenLayer:                y_h.append(hidden_neuron.feedForward(y_i))            # 输出层的标签：预测标签            y_o = []            for output_neuron in self.outputLayer:                y_o.append(output_neuron.feedForward(y_h))            # 进行预测，统计预测正误            forecastLabel = y_o.index(np.array(y_o).max())            if forecastLabel == train_labels[m]:                T+=1            else:                N+=1            # 期望值数组            trueLabels = makeTrueLabels(trueLabel=forecastLabel)            # 对输出层进行反向传播            output_sensitivities = [] # 输出层所有节点的敏感度            output_weights = [] # 输出层所有节点的权重            for i in range(10):                self.outputLayer[i].backPropagation(eta=0.1,tk=trueLabels[i],sensitivities=None,weights=None)                # 保存更新过的敏感度                output_sensitivities.append(self.outputLayer[i].sensitivity)                output_weights.append(self.outputLayer[i].weights)            # 对隐含层进行反向传播            for i in range(50):                # 隐含层每个神经元连接的10个输出层神经元的权值                linkedWeights = [arr[i] for arr in output_weights]                self.hiddenLayer[i].backPropagation(eta=0.1,tk=None,sensitivities=output_sensitivities,weights=linkedWeights)        return T, N# 传入数据集(train_images, train_labels), (test_images, test_labels) = load_data('')train_images = train_images[0:500:1]train_labels = train_labels[0:500:1]\"\"\"    定义统计信息：\"\"\"n = 100positive = []negative = []nn = NeuronNetwork(train_images=train_images,train_labels=train_labels,test_images=test_images,test_labels=test_labels)for k in range(100):    T,N = nn.iteration()    positive.append(T)    negative.append(N)\n\n该神经网络只能运行在少量的样本上，不能满足需要\n\n\n改进思路：矩阵运算之前思路中更新的过程太慢，每次都要一张一张图片传入，同时大量的对象也拖慢了运算的速度，因此我取消了神经元类，而在神经网络中保存两个矩阵，分别代表从输入层到隐含层与从隐含层到输出层的权值。\n\n\n神经网络类通过输入层节点数、隐含层节点数、输出层节点数、学习率进行初始化，同时初始化两个权值矩阵以及一个偏置数组。\n# 制作一个神经网络算法的类，其名为神经网络，相当于函数库，直接进行调用里面的函数即可。class NeuralNetwork:    def __init__(self,inputNeurons,hiddenNeurons,outputNeurons,lr):        \"\"\"        神经网络构造方法        :param inputNeurons:输入层神经元个数        :param hiddenNeurons:隐含层神经元个数        :param outputNeurons:输出层神经元个数        :param lr:学习率        \"\"\"        self.iNeuron_num = inputNeurons        self.hNeuron_num = hiddenNeurons        self.oNeuron_num = outputNeurons        self.learnRate = lr # 学习率        self.f = lambda x: ssp.expit(x) # 设置激活函数f为Sigmod(x)激活函数        # 设置输入层与隐藏层直接的权重关系矩阵以及隐藏层与输出层之间的权重关系矩阵,初始值为正态分布        self.weights_i_h = np.random.normal(0.0, 1/np.sqrt(hiddenNeurons), (self.hNeuron_num, self.iNeuron_num))        self.weights_h_o = np.random.normal(0.0, 1/np.sqrt(hiddenNeurons), (self.oNeuron_num, self.hNeuron_num))        # 偏置初始化        self.bias = np.random.normal(0, 0.5, 2)\n\n对输入的图片与标签进行两步处理：\n\n归一化\n\n对于图片将所有的灰度(0-255)全部映射到(0.01-0.99)上\n对于每一个表示分类结果的标签(0-9)重新构造一个长度为10的数组，下标对应每个标签，若真实分类标签为8，则数组[7] = 0.99，其他位置的值为0.01，近似表示该图片真实标签的概率。\n\n  # 对60000张图片进行遍历for i in range(60000):    # 测试集的28*28矩阵转化为784的一维数组    img = train_images[i].reshape(train_images.shape[1]*train_images.shape[2])    # 进行归一化：除以255，再乘以0.99，最后加上0。01，保证所有的数据都在0.01到1.00之间    train_matrix[:,i] = (img/255.0)*0.99+0.01    # 建立准确输出结果矩阵，对应的位置标签数值为0.99，其他位置为0.01    # 第i张图片代表第i列，行数代表正确的标签    train_labels_matrix[train_labels[i],i] = 0.99    # 对10000章测试集图片进行处理for i in range(10000):    # 训练集的28*28矩阵转化为784的一维数组    test_img = test_images[i].reshape(test_images.shape[1] * test_images.shape[2])    # 更新输入到神经网络中的训练集矩阵    test_matrix[:,i] = (test_img/255.0)*0.99+0.01    # 建立准确输出结果矩阵，对应的位置标签数值为0.99，其他位置为0.01    test_label_matrix[test_labels[i],i] = 0.99\nReshape：将输入的图片灰度与标签从新组合\n  \n\n整个前馈的矩阵运算的过程如图：\n\n\nBP的过程主要分为两个部分：\n\n对于输出层到隐含层：\n  计算f'(net) = f(net)*(1-f(net))计算loss'(zk)=-(tk-zk)计算对于每张图片该神经元的敏感度(10,60000):sensitivity = -f'(output_f_net)*loss'(zk) = (tk-zk)*f(output_f_net)*(1-f(output_f_net))[矩阵对应位置相乘]更新权重:学习率*敏感度(10,60000) @ 50个隐含层层神经元的输出(60000,50)\n对于隐含层到输入层：\n  计算对于每张图片50个隐含层神经元的每一个神经元的从输出层传入的敏感度（50*60000)：输出层敏感度的加权和*f'(net)更新权重\n\n完整的训练过程如下：\ndef train(self,featuresMatrix,targetMatrix,iterateNum):    \"\"\"    神经网络一次训练    :param featuresMatrix: 784*60000的图片灰度矩阵 也是隐含层的输入    :param targetMatrix: 10*60000的期望值矩阵 tk    :param iterateNum: 迭代序号    :return: 返回训练正确率    \"\"\"    T = 0    N = 0    # 前馈 feedforward    # 隐藏层net（50*60000）计算    hidden_net = (self.weights_i_h @ featuresMatrix)+self.bias[0]    # 隐藏层输出f(hidden_net)    hidden_f_net = self.f(hidden_net)    # 输出层net(10*60000) 计算    output_net = (self.weights_h_o @ hidden_f_net)+self.bias[1]    # 输出层输出f(output_net) zk    output_f_net = self.f(output_net)    # 统计网络预测正确率    for imgIndex in range(60000):        # 返回输出层10个神经元最大值下标 与 预测标签        if output_f_net[:,imgIndex].argmax() == targetMatrix[:,imgIndex].argmax():            T+=1        else:            N+=1    print(\"第\"+iterateNum+\"次训练集迭代正确率：\"+str(T/60000))    # 反向传播运算 backPropagation    # 对于输出层到隐含层    output_errors = targetMatrix - output_f_net    # 计算f'(net) = f(net)*(1-f(net))    # 计算loss'(zk)=-(tk-zk)    # 计算对于每张图片该神经元的敏感度(10,60000) sensitivity = -f'(net)*loss'(zk) = (tk-zk)*f(net)*(1-f(net))[矩阵对应位置相乘]    sensitivities = output_errors * output_f_net * (1.0 - output_f_net)    # 更新权重 学习率* 敏感度(10,60000) @ 50个隐含层层神经元的输出(60000,50)    self.weights_h_o += self.learnRate * (sensitivities @ hidden_f_net.T)    # 对于隐含层到输入层    # 计算对于每张图片50个隐含层神经元的每一个敏感度（50*60000） = 输出层敏感度的加权和*f'(net)    hidden_sensitivities = (self.weights_h_o.T @ sensitivities) * hidden_f_net * (1 - hidden_f_net)    # 更新权重    self.weights_i_h += self.learnRate * ( hidden_sensitivities @ featuresMatrix.T)        return T/60000\n\n预测的过程就是进行一遍前馈的过程：\ndef test(self,testMatrix,targetMatrix,iterateNum):    \"\"\"    利用神经网络对训练集进行一次测试    :param testMatrix: 784*10000的灰度矩阵    :param targetMatrix: 10*10000的预测标签矩阵    :param iterateNum: 迭代序号    :return: 返回训练正确率    \"\"\"    T = 0    N = 0    # 前馈 feedforward    # 隐藏层net（50*60000）计算    hidden_net = (self.weights_i_h @ testMatrix) + self.bias[0]    # 隐藏层输出f(hidden_net)    hidden_f_net = self.f(hidden_net)    # 输出层net(10*60000) 计算    output_net = (self.weights_h_o @ hidden_f_net) + self.bias[1]    # 输出层输出f(output_net) zk    output_f_net = self.f(output_net)    # 统计网络预测正确率    for imgIndex in range(10000):        # 返回输出层10个神经元最大值下标 与 预测标签        if output_f_net[:, imgIndex].argmax() == targetMatrix[:, imgIndex].argmax():            T += 1        else:            N += 1    print(\"第\" + iterateNum + \"次测试集迭代正确率：\" + str(T / 10000))    return T / 10000\n\n调试与结果展示代码如下：\n# 导入数据(train_images, train_labels), (test_images, test_labels) = load_data('')# 输出图片# plt.imshow(train_images[0], cmap='Greys', interpolation='None')# 初始化输入数据矩阵train_matrix = np.zeros((784,60000))test_matrix = np.zeros((784,10000))# 初始化输出层期望值矩阵train_labels_matrix = np.zeros((10,60000))+0.01test_label_matrix = np.zeros((10,10000))+0.01# 对60000张图片进行遍历for i in range(60000):    # 测试集的28*28矩阵转化为784的一维数组    img = train_images[i].reshape(train_images.shape[1]*train_images.shape[2])    # 进行归一化：除以255，再乘以0.99，最后加上0。01，保证所有的数据都在0.01到1.00之间    train_matrix[:,i] = (img/255.0)*0.99+0.01    # 建立准确输出结果矩阵，对应的位置标签数值为0.99，其他位置为0.01    # 第i张图片代表第i列，行数代表正确的标签    train_labels_matrix[train_labels[i],i] = 0.99# 对10000章测试集图片进行处理for i in range(10000):    # 训练集的28*28矩阵转化为784的一维数组    test_img = test_images[i].reshape(test_images.shape[1] * test_images.shape[2])    # 更新输入到神经网络中的训练集矩阵    test_matrix[:,i] = (test_img/255.0)*0.99+0.01    # 建立准确输出结果矩阵，对应的位置标签数值为0.99，其他位置为0.01    test_label_matrix[test_labels[i],i] = 0.99# 学习率learn_rate = 0.000025# 迭代次数epochs = 100# 初始化神经网络nn = NeuralNetwork(784,50,10,learn_rate)# 准确率数组train_accuracy = []test_accuracy = []# 进行迭代for i in range(epochs):    train_accuracy.append( nn.train(train_matrix,train_labels_matrix,str(i)) )    test_accuracy.append( nn.test(test_matrix,test_label_matrix,str(i)) )# 画图plt.plot(range(1,epochs+1),train_accuracy,'y')plt.plot(range(1,epochs+1),test_accuracy,'g')plt.legend(labels = ('train accuracy', 'test accuracy'), loc = 'lower right') # legend placed at lower rightplt.title(\"learn rate: \"+str(learn_rate))plt.xlabel('iteration')plt.ylabel('accuracy')plt.show()\n\n实验结果分析当学习率为0.1时，由于学习率过大导致更新的权重过大使得权重数组全为负值，继而在之后的迭代中计算激活能时是一个极大的复数，带入sigmod函数趋近于0，从而导致权值不再更新，正确率维持在0.1更新。\n\n\n\n\n解决的方法是降低学习率，不让权值更新的过快，将学习率降低为0.000025时，迭代50次，整个模型有很大的优化：\n\n\n提高迭代次数至200次，预测正确率可以达到75%左右：\n\n\n参考：【1】神经网络基础与反向传播_Sunburst7的博客-CSDN博客\n【2】识别MNIST数据集之（二）：用Python实现神经网络_superCally的专栏-CSDN博客\n【3】 用python创建的神经网络–mnist手写数字识别率达到98%_学习机器学习-CSDN博客_mnist手写数字识别python\n【4】利用Python对MNIST手写数据集进行数字识别（初学者入门级）_仲子_real-CSDN博客_mnist手写数字识别python\n","categories":["机器学习"],"tags":["机器学习","python"]},{"title":"实验：参数估计与非参数估计","url":"/2022/01/10/%E5%AE%9E%E9%AA%8C%EF%BC%9A%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1%E4%B8%8E%E9%9D%9E%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1/","content":"最大似然估计实验要求\n\n使用上面给出的三维数据： \n\n编写程序，对类1和类2中的三个特征𝑥𝑖分别求解最大似然估计的均值𝜇̂和方差。 \n\n编写程序，处理二维数据的情形𝑝(𝑥)~𝑁(µ, Σ)。对类1和类2中任意两个特征的组合分别求解最大似然估计的均值𝜇̂和方差（每个类有3种可能）。\n\n编写程序，处理三维数据的情形𝑝(𝑥)~𝑁(µ, Σ)。对类1和类2中三个特征求解最大似然估计的均值𝜇̂和 方差𝛴。 \n\n假设该三维高斯模型是可分离的，即，编写程序估计类1和类2中的均值和协方差矩阵中的参数。 \n\n比较前 4 种方法计算出来的每一个特征的均值的异同，并加以解释。 \n\n比较前 4 种方法计算出来的每一个特征的方差的异同，并加以解释。\n\n\n实验思路根据最大似然估计的原理，可以推导出：均值的最大似然估计就是样本的均值，而协方差的最大似然估计是n个的算术平均。实际上对方差的最大似然估计是有偏的估计，样本的协方差矩阵，而我们估计的方差是，具体原理可以看：参数估计—最大似然估计与贝叶斯估计\n\n\n对于任意一个多元的高斯分布，这里的多元就对应着数据的多特征（例如本次实验中的x1,x2,x3），此高斯分布的采样是以列向量的形式，每行的值为一个随机变量，因此计算统计属性：\n\n均值：分别计算每个特征的均值，以向量的形式输出，即均值向量\n方差：数据集中所有向量（列向量）计算，在求和取平均\n\n当高斯模型是可分离的时，说明每个特征（随机变量）相互独立，则任意两个特征的协方差为0（Cov(x1,x2)=0）,因此协方差矩阵的形式如下：\n代码实现数据以DataFrame的形式存储，计算均值向量的函数：\n# 通用的计算一个数据集的平均向量def calculateAvg(vectors:pd.DataFrame):    # 创建一个空Series存储平均数    avg = pd.Series(index=vectors.columns,dtype=float)    for column in vectors.columns:        # 分别计算每个特诊的平均值        avg[column] = vectors[column].mean()    return np.array(avg)\n\n计算协方差矩阵的函数：\n# 通用的计算一个数据集的估计协方差矩阵：对每个向量求其协方差矩阵再求和取平均# 返回一个协方差矩阵，训练集内是一维向量的话返回的矩阵只有一个元素def calculateCov(vectors:pd.DataFrame):    # 计算数据集的均值列向量    mu = np.matrix(calculateAvg(vectors)).T    # 获取训练集中每个随机变量的维度    dimension = vectors.shape[1]    Cov = np.zeros((dimension,dimension))    for index,row in vectors.iterrows():        # 取出训练集中的每一个数据，形式为列向量        xi = np.matrix(row).T        diff = xi - mu        Cov+=diff*diff.T    # 取平均    return Cov/vectors.shape[0]\n\n利用DataFrame[[特征1,特征2,..]]来提取训练集中的某几个特征，分别实现计算。\n\n问（1）编写程序，对类1和类2中的三个特征𝑥𝑖分别求解最大似然估计的均值𝜇̂和方差。 \n  # 创建数据帧trainSet_1 = pd.read_csv('w1.csv')trainSet_2 = pd.read_csv('w2.csv')# (1)对类 1 和类 2 中的三个特征𝑥𝑖分别求解最大似然估计的均值𝜇̂和方差𝜎2。print(\"(1): \")print(\"类1：\")trainSet_1_x1 = trainSet_1['x1'].to_frame()print(\"x1的最大似然估计:μ：\"+str(calculateAvg(trainSet_1_x1))+\" 𝜎^2: \"+str(calculateCov(trainSet_1_x1)))trainSet_1_x2 = trainSet_1['x2'].to_frame()print(\"x2的最大似然估计:μ：\"+str(calculateAvg(trainSet_1_x2))+\" 𝜎^2: \"+str(calculateCov(trainSet_1_x2)))trainSet_1_x3 = trainSet_1['x3'].to_frame()print(\"x3的最大似然估计:μ：\"+str(calculateAvg(trainSet_1_x3))+\" 𝜎^2: \"+str(calculateCov(trainSet_1_x3)))print(\"------------------------------------------------------------------------------\")print(\"类2：\")trainSet_2_x1 = trainSet_2['x1'].to_frame()print(\"x1的最大似然估计:μ：\"+str(calculateAvg(trainSet_2_x1))+\" 𝜎^2: \"+str(calculateCov(trainSet_2_x1)))trainSet_2_x2 = trainSet_2['x2'].to_frame()print(\"x2的最大似然估计:μ：\"+str(calculateAvg(trainSet_2_x2))+\" 𝜎^2: \"+str(calculateCov(trainSet_2_x2)))trainSet_2_x3 = trainSet_2['x3'].to_frame()print(\"x3的最大似然估计:μ：\"+str(calculateAvg(trainSet_2_x3))+\" 𝜎^2: \"+str(calculateCov(trainSet_2_x3)))\n问（2）编写程序，处理二维数据的情形𝑝(𝑥)~𝑁(µ, Σ)。对类1和类2中任意两个特征的组合分别求解最大似然估计的均值𝜇̂和方差（每个类有3种可能）。\n  # 创建数据帧trainSet_1 = pd.read_csv('w1.csv')trainSet_2 = pd.read_csv('w2.csv')# (2)处理二维数据的情形𝑝𝑝(𝑥)~𝑁(µ, Σ)。对类 1 和类 2 中任意两个特征的组合分别求解最大似然估计的均值𝜇̂和方差𝛴𝛴（每个类有3种可能）。print(\"(2): \")print(\"类1：\")trainSet_1_x1x2 = trainSet_1[['x1','x2']]print(\"(x1,x2)的最大似然估计:\")print(\"μ：\"+str(calculateAvg(trainSet_1_x1x2)))print(\"𝜎^2: \")print(calculateCov(trainSet_1_x1x2))trainSet_1_x1x3 = trainSet_1[['x1','x3']]print(\"(x1,x3)的最大似然估计:\")print(\"μ：\"+str(calculateAvg(trainSet_1_x1x3)))print(\"𝜎^2:\")print(calculateCov(trainSet_1_x1x3))trainSet_1_x2x3 = trainSet_1[['x2','x3']]print(\"(x2,x3)的最大似然估计:\")print(\"μ：\"+str(calculateAvg(trainSet_1_x2x3)))print(\"𝜎^2: \")print(calculateCov(trainSet_1_x2x3))print(\"------------------------------------------------------------------------------\")print(\"类2：\")trainSet_2_x1x2 = trainSet_2[['x1','x2']]print(\"(x1,x2)的最大似然估计:\")print(\"μ：\"+str(calculateAvg(trainSet_2_x1x2)))print(\"𝜎^2: \")print(calculateCov(trainSet_2_x1x2))trainSet_2_x1x3 = trainSet_2[['x1','x3']]print(\"(x1,x3)的最大似然估计:\")print(\"μ：\"+str(calculateAvg(trainSet_2_x1x3)))print(\"𝜎^2: \")print(calculateCov(trainSet_2_x1x3))trainSet_2_x2x3 = trainSet_2[['x2','x3']]print(\"(x2,x3)的最大似然估计:\")print(\"μ：\"+str(calculateAvg(trainSet_2_x2x3)))print(\"𝜎^2: \")print(calculateCov(trainSet_2_x2x3))\n（3）编写程序，处理三维数据的情形𝑝(𝑥)~𝑁(µ, Σ)。对类1和类2中三个特征求解最大似然估计的均值𝜇̂和 方差𝛴。 \n  # 创建数据帧trainSet_1 = pd.read_csv('w1.csv')trainSet_2 = pd.read_csv('w2.csv')# (3)编写程序，处理三维数据的情形𝑝(𝑥)~𝑁(µ, Σ)。对类 1 和类 2 中三个特征求解最大似然估计的均值𝜇̂和方差𝛴𝛴print(\"(3)\")print(\"类1\")print(\"(x1,x2,x3)的最大似然估计: µ\"+str(calculateAvg(trainSet_1)))print(\"Σ:\")print(calculateCov(trainSet_1))print(\"------------------------------------------------------------------------------\")print(\"类2\")print(\"(x1,x2,x3)的最大似然估计: µ\"+str(calculateAvg(trainSet_2)))print(\"Σ:\")print(calculateCov(trainSet_2))\n（4）假设该三维高斯模型是可分离的，即，编写程序估计类1和类2中的均值和协方差矩阵中的参数。 \n  # 创建数据帧trainSet_1 = pd.read_csv('w1.csv')trainSet_2 = pd.read_csv('w2.csv')# 取出每一个特征trainSet_1_x1 = trainSet_1['x1'].to_frame()trainSet_1_x2 = trainSet_1['x2'].to_frame()trainSet_1_x3 = trainSet_1['x3'].to_frame()trainSet_2_x1 = trainSet_2['x1'].to_frame()trainSet_2_x2 = trainSet_2['x2'].to_frame()trainSet_2_x3 = trainSet_2['x3'].to_frame()# (4)# 因为该模型是可分离的，所以各个特征之间相互独立，每个训练样本向量(x1,x2,x3)的Cov(xi,xj)=0 所以协方差是一个对角矩阵，对角线即为一维数据的方差。print(\"类1\")print(\"(x1,x2,x3)的最大似然估计:\")print(\"µ\"+str(calculateAvg(trainSet_1)))Cov_1 = np.zeros((3,3))Cov_1[0, 0] = calculateCov(trainSet_1_x1)Cov_1[1, 1] = calculateCov(trainSet_1_x2)Cov_1[2, 2] = calculateCov(trainSet_1_x3)print(\"Σ:\")print(Cov_1)print(\"----------------------------------------------------------\")print(\"类2\")print(\"(x1,x2,x3)的最大似然估计:\")print(\"µ\"+str(calculateAvg(trainSet_2)))Cov_2 = np.zeros((3,3))Cov_2[0, 0] = calculateCov(trainSet_2_x1)Cov_2[1, 1] = calculateCov(trainSet_2_x2)Cov_2[2, 2] = calculateCov(trainSet_2_x3)print(\"Σ:\")print(Cov_2)\n（5）（6）比较前 4 种方法计算出来的每一个特征的均值与方差的异同，并加以解释。 \n  均值的计算与向量维度无关，都是每一维数据求和再除以n。\n  因为该模型是可分离的，所以各个特征之间相互独立，每个训练样本向量(x1,x2,x3)的Cov(xi,xj)=0 所以协方差是一个对角矩阵，除对角线外其他处的值为0，对角线即为一维数据的方差。\n\n\n实验结果\n问题1\n \n问题2\n (2): 类1：(x1,x2)的最大似然估计:μ：[-0.0709 -0.6047]𝜎^2: [[0.90617729 0.56778177] [0.56778177 4.20071481]](x1,x3)的最大似然估计:μ：[-0.0709 -0.911 ]𝜎^2:[[0.90617729 0.3940801 ] [0.3940801  4.541949  ]](x2,x3)的最大似然估计:μ：[-0.6047 -0.911 ]𝜎^2: [[4.20071481 0.7337023 ] [0.7337023  4.541949  ]]------------------------------------------------------------------------------类2：(x1,x2)的最大似然估计:μ：[-0.0426  0.4299]𝜎^2: [[ 0.06478984 -0.01184426] [-0.01184426  0.04597009]](x1,x3)的最大似然估计:μ：[-0.0426   0.00372]𝜎^2: [[ 0.06478984 -0.00306033] [-0.00306033  0.00726551]](x2,x3)的最大似然估计:μ：[0.4299  0.00372]𝜎^2: [[0.04597009 0.00850987] [0.00850987 0.00726551]]\n问题3\n \n问题4\n \n\nParzen窗实验要求\n\nParzen 窗估计： 使用上面表格中的数据进行 Parzen 窗估计和设计分类器。窗函数为一个球形的高斯函数如下所示：\n\n\n编写程序，使用 Parzen 窗估计方法对任意一个的测试样本点𝑥进行分类。对分类器的训练则使用表格中的三维数据。令h = 1，分类样本点为，， 。\n实验思路实验给出的训练集数据分类三个类别，记为w1,w2,w3，我们分别对这三类数据计算每个训练集样本的关于样本测试点的窗函数值，在求和取平均，计算出估计的后验概率。\n由于先验知识我的得到，这三类的先验概率应相等，所以由最大后验概率决策变为最大似然决策，根据以下公式计算出每个类的估计类条件概率密度，比大小判断即可。\n\n值得注意的是，Parzen窗是将测试样本点放在窗的中心，让训练集中的每个数据去与窗中心比对，也就是代表测试集数据，代表训练集数据\n代码实现\n11行定义了一个窗函数计算函数，输入一个测试样本点、一个训练集样本点以及窗宽，输出窗函数的值。\n21行定义了一个求和取平均的函数，用于计算每个类的类条件概率密度\n31行定义了一个分类器，传入测试点，通过已经输入好的训练集对该测试数据进行分类\n\nimport pandas as pdimport numpy as npimport math# 导入训练集数据trainSet_1 = pd.read_csv('w1.csv')trainSet_2 = pd.read_csv('w2.csv')trainSet_3 = pd.read_csv('w3.csv')# 计算sample测试数据在训练数据trainSample下的窗函数def window(sample:pd.Series,trainSample:pd.Series,h):    # 将Series转化为列向量    vector_s = np.matrix(sample).T    vector_ts = np.matrix(trainSample).T    # 计算 x-xi    diff = vector_s - vector_ts    # 返回窗函数值    return math.exp(-diff.T*diff/(2*h**2))# Parzen窗方法估计该类的条件概率密度def Parzen(sample:pd.Series,trainSet:pd.DataFrame):    # 初始化似然    likelihood = 0.0    for index, row in trainSet.iterrows():        # 对样本的每个点计算窗函数(h=1)，累加        likelihood+=window(sample,row,1)    likelihood = likelihood/10    # 返回估计的后验概率    return likelihood# 该实验的Parzen窗分类器，训练集中每个类的先验概率相等，因此后验概率就等于类条件概率密度def ParzenClassifier(sample:pd.Series):    posterior_1 = Parzen(sample, trainSet_1)    posterior_2 = Parzen(sample, trainSet_2)    posterior_3 = Parzen(sample, trainSet_3)    print(sample)    print(\"p(w1): \"+  str(posterior_1))    print(\"p(w2): \" + str(posterior_2))    print(\"p(w3): \" + str(posterior_3))    if posterior_1&gt;posterior_2:        if posterior_1&gt;posterior_3:            print(\"Sample belong 类1\")        else:            print(\"Sample belong 类3\")    else:        if posterior_2&gt;posterior_3:            print(\"Sample belong 类2\")        else:            print(\"Sample belong 类3\")    print(\"-------------------------------------\")ParzenClassifier(pd.Series([0.5,1.0,0.0]))ParzenClassifier(pd.Series([0.31,1.51,-0.50]))ParzenClassifier(pd.Series([-0.3,0.44, -0.1]))\n\n实验结果\n\n可以看到这三个测试数据都属于类2\nK近邻实验要求\n\nk-近邻概率密度估计： 对上面表格中的数据使用k-近邻方法进行概率密度估计： \n\n编写程序，对于一维的情况，当有 n 个数据样本点时，进行k-近邻概率密度估计。 对表格中的类3的特征𝑥1，用程序画出当 k=1，3，5 时的概率密度估计结果。  \n编写程序，对于二维的情况，当有 n 个数据样本点时，进行k-近邻概率密度估计。 对表格中的类2的特征(𝑥1, 𝑥2)𝑡，用程序画出当 k=1，3，5 时的概率密度估计结果。 \n编写程序，对表格中的3个类别的三维特征，使用k-近邻概率密度估计方法。并且 对下列点处的概率密度进行估计： (-0.41,0.82,0.88)𝑡，(0.14,0.72, 4.1)𝑡，(-0.81,0.61,  -0.38)𝑡\n\n实验思路\n实验的核心公式就是（3）式，给定一个测试数据点，以测试数据点为中心，我们分别计算从该点到训练集中样本点的数据的距离作为度量的标准，排序。然后选出距离测试数据点第k近的样本点距离，计算出包括k个训练集数据点的超立方体体积，带入公式计算。具体的针对不同维度：\n\n一维：，超立方体的体积就是以测试点为中心，的线段长度。\n二维：，超立方体体积是以测试点为中心，为半径的圆\n三维：\n\n指的注意的是，在画图的过程中，可能出现测试点与样本点重合的情况，这时最好在分母加上一个极小项防止/0。\n当我们估计出类条件概率密度后，因为由表中数据得到，每个类别的训练数据数量相等先验概率相同，因此对于每个测试数据只需要计算三种情况下的类条件概率密度，再用最大类条件概率密度估计进行决策即可。\n代码实现及结果一维情况编写了一个1维KNN方法计算一个测试数据集的概率密度，输入测试数据（一维）、训练集、K值。输出概率密度\n# 对于一维的情况，当有 n 个数据样本点时，进行k-近邻概率密度估计，# 对于类3_x3特征，估计任意一个点关于类3的类条件概率密度import numpy as npimport pandas as pdimport matplotlib.pyplot as plt# 定义1维KNN方法计算一个测试数据集的概率密度，输入测试数据、训练集、K值 输出概率密度def one_dimension_KNN(testData:float,trainSet:pd.Series,k:int):    distance = []    for i in range(trainSet.shape[0]):        # 计算测试数据点 与 训练集中每个样本点的距离        d = np.abs(testData-trainSet[i])        distance.append(d)    # 距离数组进行排序 提取出第k个数据    distance.sort()    posterior = (k/trainSet.shape[0])/(2*distance[k-1])    return posterior# 导入实验1数据，w3类的x3特征trainSet1 = pd.read_csv('w3.csv')['x3']# 随机产生n=500个-2~2的1维随机数dimension1_randoms = np.random.uniform(-2, 2, 500)# 进行升序排序dimension1_randoms = np.sort(dimension1_randoms)# 声明三个一维数组用于存储K值不同情况下的后验概率dimension1_posterior_1 = []dimension1_posterior_3 = []dimension1_posterior_5 = []# 对随机数计算后验概率for i in range(500):    dimension1_posterior_1.append(one_dimension_KNN(dimension1_randoms[i], trainSet1, 1))    dimension1_posterior_3.append(one_dimension_KNN(dimension1_randoms[i], trainSet1, 3))    dimension1_posterior_5.append(one_dimension_KNN(dimension1_randoms[i], trainSet1, 5))# 画出三张一维的图像plt.subplot(131)plt.plot(dimension1_randoms,dimension1_posterior_1)plt.title('k=1 pdf')plt.subplot(132)plt.plot(dimension1_randoms,dimension1_posterior_3)plt.title('k=3 pdf')plt.subplot(133)plt.plot(dimension1_randoms,dimension1_posterior_5)plt.title('k=5 pdf')plt.show()\n\n实验结果如图，可以看到在k=1时，在训练集样本点周围有着明显的尖峰，表示图中充满噪音（可能测试样本只是和某个训练样本接近而不是该类，但类条件概率密度却很大）当k增大时曲线变得平滑最后收敛为一个极值，表示类条件概率密度的估计慢慢变得准确。\n\n\n二维情况定义2维KNN方法\n\n输入n个测试数据的x1特征取值矩阵与x2特征取值矩阵、训练数据集、k值。\n输出后验概率估计数组\n\n生成x1、x2特征的测试数据是在一定范围内每隔0.05采样。\n# 对于2维的情况，当有 n 个数据样本点时，进行k-近邻概率密度估计，import numpy as npimport pandas as pdimport matplotlib.pyplot as pltfrom mpl_toolkits.mplot3d import Axes3D# 定义2维knn方法(x1,x2) 输入x1特征取值矩阵与x2特征取值矩阵、训练数据集、k# x1矩阵代表二维坐标轴的所有点的x1特征值 x2矩阵代表二维坐标轴的所有点的x2特征值def two_dimension_KNN(x1:np.matrix,x2:np.matrix,trainSet:pd.DataFrame,k:int):    # 声明一个后验概率矩阵存储计算出的后验概率 行(x轴)代表x2的取值，列(y轴)代表x1取值    posteriorMatrix = np.zeros((x1.shape[0],x1.shape[1]))    # 每一个x2特征    for i in range(x1.shape[0]):        # 每一个x1特征        for j in range(x1.shape[1]):            # 存储二维的欧氏距离            distance = []            for index,row in trainSet.iterrows():                # 计算测试数据点 与 训练集中每个样本点的欧式距离                d = np.sqrt((x1[i,j]-row[0])**2+(x2[i,j]-row[1])**2)                distance.append(d)            # 距离数组进行排序 提取出第k个数据            distance.sort()            # 分母加上一个极小的数防止测试点与样本点重合导致分布为0的情况            posterior = (k / trainSet.shape[0]) / (np.pi * (distance[k - 1]**2)+np.spacing(1))            posteriorMatrix[i,j] = posterior    return posteriorMatrix# 导入实验2数据 w2类的x1,x2特征trainSet2 = pd.read_csv('w2.csv')[['x1','x2']]# 生成x1 100个 x2 80个测试数据test_x1 = np.arange(-3, 2, 0.05)test_x2 = np.arange(0, 4, 0.05)# 将x1与x2网格化matrix_x1,matrix_x2 = np.meshgrid(test_x1, test_x2)# 计算不同k值情况下网格中每个点的后验概率posterior1 = two_dimension_KNN(matrix_x1,matrix_x2,trainSet2,1)posterior3 = two_dimension_KNN(matrix_x1,matrix_x2,trainSet2,3)posterior5 = two_dimension_KNN(matrix_x1,matrix_x2,trainSet2,5)# 画图配置fig = plt.figure(figsize=(12, 6), facecolor='w')ax1 = fig.add_subplot(131, projection='3d')ax1.plot_surface(matrix_x1,matrix_x2,posterior1,                    rstride=1,  # rstride（row）指定行的跨度                    cstride=1,  # cstride(column)指定列的跨度                    cmap=plt.get_cmap('rainbow'))  # 设置颜色映射ax1.set_xlabel('x2')ax1.set_ylabel('x1')ax1.set_zlabel('likelihood')plt.title('k=1 pdf')ax2 = fig.add_subplot(132, projection='3d')ax2.plot_surface(matrix_x1,matrix_x2,posterior3,                    rstride=1,                    cstride=1,                    cmap=plt.get_cmap('rainbow'))ax1.set_xlabel('x2')ax1.set_ylabel('x1')ax1.set_zlabel('likelihood')plt.title('k=3 pdf')ax3 = fig.add_subplot(133, projection='3d')ax3.plot_surface(matrix_x1,matrix_x2,posterior5,                    rstride=1,                    cstride=1,                    cmap=plt.get_cmap('rainbow'))ax1.set_xlabel('x2')ax1.set_ylabel('x1')ax1.set_zlabel('likelihood')plt.title('k=5 pdf')plt.show()\n\n实验结果如图所示：同一维的情况，在k=1时，在训练集样本点周围有着明显的尖峰，表示图中充满噪音（可能测试样本只是和某个训练样本接近而不是该类，但类条件概率密度却很大）当k增大时曲线变得平滑最后收敛为一个极值，表示类条件概率密度的估计慢慢变得准确。\n\n\n三维情况该情况与前面的有所不同，实验要求导入三个类别全部特征，判断具体的三个测试数据属于哪个类别，因此对于每个测试数据需要计算三种情况下的类条件概率密度，再用最大类条件概率密度估计进行决策。\nimport numpy as npimport pandas as pd# 导入实验3数据 三个类别的全部特征trainSet3_1 = pd.read_csv('w1.csv')trainSet3_2 = pd.read_csv('w2.csv')trainSet3_3 = pd.read_csv('w3.csv')trainSet = [trainSet3_1, trainSet3_2, trainSet3_3]# 三维的KNN方法，该方法输入一个三维列向量，k值；输出该向量的类别def three_dimension_KNN(testData:np.matrix,k:int):    # 声明距离数组用于保存距离    distance = [[],[],[]]    posterior = []    for i in range(3):        # 对每类的数据集计算测试数据点与训练集中每个样本点的距离        for j in range(10):            d = np.sqrt((testData[0,0]-trainSet[i].iloc[j]['x1'])**2 +                        (testData[1,0]-trainSet[i].iloc[j]['x2'])**2 +                        (testData[2,0]-trainSet[i].iloc[j]['x3'])**2)            distance[i].append(d)        # 距离数组进行排序 提取出第k个数据        distance[i].sort()        V = 4 * np.pi * (distance[i][k - 1] ** 3) / 3        posterior.append(k/10/V)    print(\"类条件概率密度数组:\"+str(posterior))    return posterior.index(max(posterior))# [0.14],[0.72],[4.1]  [-0.81],[0.61],[-0.38]]print(\"(-0.41,0.82,0.88)属于w\"+str(three_dimension_KNN(np.matrix([[-0.41],[0.82],[0.88]]),3)))print(\"(0.14,0.72,4.1)属于w\"+str(three_dimension_KNN(np.matrix([[0.14],[0.72],[4.1]]),3)))print(\"(-0.81,0.61,-0.38)属于w\"+str(three_dimension_KNN(np.matrix([[-0.81],[0.61],[-0.38]]),3)))\n\n分类的结果如图：\n\n\n\n\nKNN实战现有一数据集存放在 e2.txt 中，共有 1000 条数据。e2.txt 中数据格式如下图所示： \n\n\n三个特征： 1.每年的出行里程 2.玩游戏所占用的时间百分比 3.每三天喝的牛奶总升数。三个标签： 1.不喜欢 2.一般 3.喜欢 \n用学过的 KNN 方法 来构建一个分类器，判断一个样本所属的类别\n实验要求\n数据预处理 \n\n将 e2.txt 中的数据处理成可以输入给模型的格式 \n是否还需要对特征值进行归一化处理？目的是什么？\n\n\n数据可视化分析：将预处理好的数据以散点图的形式进行可视化，通过直观感觉总结规律，感受KNN模型思想与人类经验的相似之处。 \n\n构建 KNN 模型并测试 \n\n输出测试集各样本的预测标签和真实标签，并计算模型准确率。 \n\n选择哪种距离更好？欧氏还是马氏？ \n\n改变数据集的划分以及 k 的值，观察模型准确率随之的变化情况。 注意：选择训练集与测试集的随机性\n\n\n\n使用模型构建可用系统 利用构建好的 KNN 模型实现系统，输入为新的数据的三个特征，输出为预测的类别。\n\n\n实验思路KNN模型的核心方法还是之前构造的三维KNN方法，简单的修改了一点输出：我们这里拿欧式距离度量\n# 三维的KNN方法，该方法输入一个三维列向量，k值；输出该向量的类别def three_dimension_KNN(testData:np.matrix,k:int):    # 声明距离数组用于保存距离    distance = [[],[],[]]    posterior = []    # 对三种预测标签分别计算后验概率    for i in range(3):        # 对每类的数据集计算测试数据点与训练集中每个样本点的距离        for j in range(trainSet[i].shape[0]):            # 计算欧氏距离            d = np.sqrt((testData[0,0]-trainSet[i].iloc[j]['mileage'])**2 +                        (testData[1,0]-trainSet[i].iloc[j]['game'])**2 +                        (testData[2,0]-trainSet[i].iloc[j]['milk'])**2)            distance[i].append(d)        # 距离数组进行排序 提取出第k个数据        distance[i].sort()        V = 4 * np.pi * (distance[i][k - 1] ** 3) / 3        posterior.append(k/10/V)    print(\"概率密度数组：\"+str(posterior))    if posterior.index(max(posterior)) == 0:        return 'largeDoses'    elif posterior.index(max(posterior)) == 1:        return 'smallDoses'    else:        return 'didntLike'\n\n先导入实验数据：\ndata = pd.DataFrame(columns=['mileage','game','milk','isLike'])# 读取文件with open('e2.txt','r') as f:    # 按行读取    content = f.readlines()    for line in content:        # 按照'\\t'分割        newLine = pd.Series(line.split('\\t'),index=['mileage','game','milk','isLike'])        # 去除预测标签末尾的'\\n'        newLine['isLike'] = newLine['isLike'].strip('\\n')        data = data.append(newLine,ignore_index=True)\n\n由于每个特征的度量值不同，有的特征取值很大，有的特征取值很小，如果直接进行计算欧式具体不合理，我们需要进行归一化的处理：使用最小归一化的方法，将所有的特征值映射到[0,1]区间上。计算公式如下：实际过程我们使用一个库：\n# 归一化features = data.iloc[:,0:3]features = MinMaxScaler().fit_transform(features)data.iloc[:,0:3] = features # 覆盖原来数据\n\n然后是对训练集与测试集的划分，因为有选择训练集与测试集的随机性，我们采用留出法：直接将原数据集划分为两个互斥的数据集，即训练集与测试集。具体的就是每隔10行选取一行作为测试集，余下的数据作为训练集，同时按照预测标签进行分组。\n# 每隔10行选取一行作为测试集，余下的数据作为训练集,按照预测标签进行分组testSet = data.iloc[::10,:]for index,row in testSet.iterrows():    data.drop(index=index,inplace=True)group = data.groupby('isLike')trainSet = [group.get_group('largeDoses'),group.get_group('smallDoses'),group.get_group('didntLike')]\n\n最后进行预测，画图即可。\n实验结果与思考完整代码如下：\nimport numpy as npimport pandas as pdfrom sklearn.preprocessing import MinMaxScalerimport matplotlib.pyplot as plt# 三维的KNN方法，该方法输入一个三维列向量，k值；输出该向量的类别def three_dimension_KNN(testData:np.matrix,k:int):    # 声明距离数组用于保存距离    distance = [[],[],[]]    posterior = []    # 对三种预测标签分别计算后验概率    for i in range(3):        # 对每类的数据集计算测试数据点与训练集中每个样本点的距离        for j in range(trainSet[i].shape[0]):            # 计算欧氏距离            d = np.sqrt((testData[0,0]-trainSet[i].iloc[j]['mileage'])**2 +                        (testData[1,0]-trainSet[i].iloc[j]['game'])**2 +                        (testData[2,0]-trainSet[i].iloc[j]['milk'])**2)            distance[i].append(d)        # 距离数组进行排序 提取出第k个数据        distance[i].sort()        V = 4 * np.pi * (distance[i][k - 1] ** 3) / 3        posterior.append(k/10/V)    print(\"概率密度数组：\"+str(posterior))    if posterior.index(max(posterior)) == 0:        return 'largeDoses'    elif posterior.index(max(posterior)) == 1:        return 'smallDoses'    else:        return 'didntLike'data = pd.DataFrame(columns=['mileage','game','milk','isLike'])# 读取文件with open('e2.txt','r') as f:    # 按行读取    content = f.readlines()    for line in content:        # 按照'\\t'分割        newLine = pd.Series(line.split('\\t'),index=['mileage','game','milk','isLike'])        # 去除预测标签末尾的'\\n'        newLine['isLike'] = newLine['isLike'].strip('\\n')        data = data.append(newLine,ignore_index=True)# 归一化features = data.iloc[:,0:3]features = MinMaxScaler().fit_transform(features)data.iloc[:,0:3] = features # 覆盖原来数据# 每隔10行选取一行作为测试集，余下的数据作为训练集,按照预测标签进行分组testSet = data.iloc[::10,:]for index,row in testSet.iterrows():    data.drop(index=index,inplace=True)group = data.groupby('isLike')trainSet = [group.get_group('largeDoses'),group.get_group('smallDoses'),group.get_group('didntLike')]# 进行预测T = 0 # 预测正确的个数F = 0 # 预测错误的个数forecast = []for index,row in testSet.iterrows():    columnVector = np.matrix([        [row[0]],        [row[1]],        [row[2]]    ])    forecast.append(three_dimension_KNN(columnVector,3))    if forecast[int(index/10)] == row[3]:        T+=1    else:        F+=1print(\"正确率:\"+str(T/(T+F)))# 给测试集添加预测标签列testSet['forecast'] =  forecast# 画图fig = plt.figure(figsize=(12, 6), facecolor='w')ax1 = plt.axes(projection='3d')ax1.legend(loc='best') # 添加图例ax1.scatter3D(trainSet[0]['mileage'],trainSet[0]['game'],trainSet[0]['milk'],c='r',label='largeDoses')ax1.scatter3D(trainSet[1]['mileage'],trainSet[1]['game'],trainSet[1]['milk'],c='y',label='smallDoses')ax1.scatter3D(trainSet[2]['mileage'],trainSet[2]['game'],trainSet[2]['milk'],c='b',label='didntLike')for index,row in testSet.iterrows():    if row[3] == row[4]:        ax1.scatter3D(row[0], row[1], row[2], c='g')    else:        ax1.scatter3D(row[0], row[1], row[2], c='k')ax1.set_xlabel('mileage')ax1.set_ylabel('game')ax1.set_zlabel('milk')plt.legend(loc='best')plt.show()\n\n\n当k=3时\n  \n\n​    \n图中绿色的点是预测正确的点，黑色的点是预测错误的点，其他颜色的点是训练集，可以看到在三种类别交汇的地方（决策边界处）有误判的出现。而在每种类别密集的地方基本没有误判的情况。\n","categories":["机器学习"],"tags":["机器学习","python"]}]